{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOOfQbhNX3CS0oo3HmwTe0Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuds/rl-atari-tennis/blob/main/%5BAtari%20Tennis%5D%20Multi-Agent%20Reinforcement%20Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Atari Tennis] Multi-Agent Reinforcement Learning"
      ],
      "metadata": {
        "id": "ob-2hGT3jNdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig"
      ],
      "metadata": {
        "id": "Fy1WJ1R-iP4V",
        "outputId": "f94b8622-ad1f-4e8e-b0ed-c24b55c19d1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.6 kB)\n",
            "Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium gymnasium[atari] pettingzoo multi-agent-ale-py autorom"
      ],
      "metadata": {
        "id": "7U_qTvGlj2oy",
        "outputId": "3d756f54-67cf-430b-df80-38911a2df7da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pettingzoo\n",
            "  Downloading pettingzoo-1.24.3-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting multi-agent-ale-py\n",
            "  Downloading multi-agent-ale-py-0.1.11.tar.gz (551 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m552.0/552.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting autorom\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Collecting shimmy<1.0,>=0.1.0 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom) (2.32.3)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (2024.8.30)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (6.4.5)\n",
            "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pettingzoo-1.24.3-py3-none-any.whl (847 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m847.8/847.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: multi-agent-ale-py\n",
            "  Building wheel for multi-agent-ale-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for multi-agent-ale-py: filename=multi_agent_ale_py-0.1.11-cp310-cp310-linux_x86_64.whl size=721820 sha256=15b41b15e46293e411b16a89a86948e46f8542981bf0fb31ffc469dc533167fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/75/64/8ad68adb0da26405c4b18c291b9c322c44d3e99c16b0f3b890\n",
            "Successfully built multi-agent-ale-py\n",
            "Installing collected packages: farama-notifications, multi-agent-ale-py, gymnasium, ale-py, shimmy, pettingzoo, autorom\n",
            "Successfully installed ale-py-0.8.1 autorom-0.6.1 farama-notifications-0.0.4 gymnasium-0.29.1 multi-agent-ale-py-0.1.11 pettingzoo-1.24.3 shimmy-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!AutoROM --accept-license"
      ],
      "metadata": {
        "id": "Le7dztPUkVz4",
        "outputId": "ff2fd2c4-e069-4b64-b779-13356479d3f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoROM will download the Atari 2600 ROMs.\n",
            "They will be installed to:\n",
            "\t/usr/local/lib/python3.10/dist-packages/AutoROM/roms\n",
            "\t/usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms\n",
            "\n",
            "Existing ROMs will be overwritten.\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/adventure.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/adventure.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/air_raid.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/air_raid.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/alien.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/alien.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/amidar.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/amidar.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/assault.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/assault.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/asterix.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/asterix.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/asteroids.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/asteroids.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/atlantis.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/atlantis.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/atlantis2.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/atlantis2.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/backgammon.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/backgammon.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/bank_heist.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/bank_heist.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/basic_math.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/basic_math.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/battle_zone.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/battle_zone.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/beam_rider.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/beam_rider.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/berzerk.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/berzerk.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/blackjack.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/blackjack.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/bowling.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/bowling.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/boxing.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/boxing.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/breakout.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/breakout.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/carnival.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/carnival.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/casino.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/casino.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/centipede.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/centipede.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/chopper_command.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/chopper_command.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/combat.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/combat.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/crazy_climber.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/crazy_climber.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/crossbow.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/crossbow.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/darkchambers.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/darkchambers.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/defender.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/defender.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/demon_attack.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/demon_attack.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/donkey_kong.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/donkey_kong.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/double_dunk.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/double_dunk.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/earthworld.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/earthworld.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/elevator_action.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/elevator_action.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/enduro.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/enduro.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/entombed.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/entombed.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/et.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/et.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/fishing_derby.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/fishing_derby.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/flag_capture.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/flag_capture.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/freeway.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/freeway.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/frogger.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/frogger.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/frostbite.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/frostbite.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/galaxian.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/galaxian.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/gopher.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/gopher.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/gravitar.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/gravitar.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/hangman.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/hangman.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/haunted_house.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/haunted_house.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/hero.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/hero.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/human_cannonball.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/human_cannonball.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/ice_hockey.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/ice_hockey.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/jamesbond.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/jamesbond.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/journey_escape.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/journey_escape.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/joust.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/joust.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/kaboom.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/kaboom.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/kangaroo.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/kangaroo.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/keystone_kapers.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/keystone_kapers.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/king_kong.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/king_kong.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/klax.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/klax.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/koolaid.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/koolaid.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/krull.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/krull.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/kung_fu_master.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/kung_fu_master.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/laser_gates.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/laser_gates.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/lost_luggage.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/lost_luggage.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/mario_bros.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/mario_bros.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/maze_craze.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/maze_craze.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/miniature_golf.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/miniature_golf.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/montezuma_revenge.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/montezuma_revenge.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/mr_do.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/mr_do.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/ms_pacman.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/ms_pacman.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/name_this_game.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/name_this_game.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/othello.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/othello.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/pacman.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/pacman.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/phoenix.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/phoenix.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/pitfall.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/pitfall.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/pitfall2.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/pitfall2.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/pong.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/pong.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/pooyan.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/pooyan.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/private_eye.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/private_eye.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/qbert.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/qbert.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/riverraid.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/riverraid.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/road_runner.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/road_runner.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/robotank.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/robotank.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/seaquest.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/seaquest.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/sir_lancelot.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/sir_lancelot.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/skiing.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/skiing.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/solaris.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/solaris.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/space_invaders.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/space_invaders.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/space_war.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/space_war.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/star_gunner.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/star_gunner.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/superman.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/superman.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/surround.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/surround.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/tennis.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/tennis.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/tetris.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/tetris.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/tic_tac_toe_3d.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/tic_tac_toe_3d.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/time_pilot.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/time_pilot.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/trondead.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/trondead.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/turmoil.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/turmoil.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/tutankham.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/tutankham.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/up_n_down.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/up_n_down.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/venture.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/venture.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/video_checkers.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/video_checkers.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/video_chess.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/video_chess.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/video_cube.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/video_cube.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/video_pinball.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/video_pinball.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/warlords.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/warlords.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/wizard_of_wor.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/wizard_of_wor.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/word_zapper.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/word_zapper.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/yars_revenge.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/yars_revenge.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/AutoROM/roms/zaxxon.bin\n",
            "Installed /usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms/zaxxon.bin\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ray[rllib] pymunk"
      ],
      "metadata": {
        "id": "iwt4C2URiRGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import platform\n",
        "import ray\n",
        "import torch\n",
        "import numpy\n",
        "import gymnasium as gym\n",
        "from pettingzoo.atari import tennis_v3\n",
        "from importlib.metadata import version\n",
        "import time"
      ],
      "metadata": {
        "id": "o89JlEiexh1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Python Version: {platform.python_version()}\")\n",
        "print(f\"Torch Version: {version('torch')}\")\n",
        "print(f\"Is Cuda Available: {torch.cuda.is_available()}\")\n",
        "print(f\"Cuda Version: {torch.version.cuda}\")\n",
        "print(f\"Gymnasium Version: {version('gymnasium')}\")\n",
        "print(f\"Numpy Version: {version('numpy')}\")\n",
        "print(f\"PettingZoo Version: {version('pettingzoo')}\")\n",
        "print(f\"Ray Version: {version('ray')}\")"
      ],
      "metadata": {
        "id": "KRrJSPuDiDN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvPXWTCHiO69"
      },
      "outputs": [],
      "source": [
        "def make_env(env_id):\n",
        "    \"\"\"\n",
        "    Creates and wraps the Atari environment.\n",
        "    \"\"\"\n",
        "    env = tennis_v3.env(render_mode=\"rgb_array\")\n",
        "    env.reset(seed=42)\n",
        "    #env = gym.make(env_id, render_mode='rgb_array')\n",
        "    env = NoopResetEnv(env, noop_max=30)\n",
        "    env = MaxAndSkipEnv(env, skip=4)\n",
        "    env = EpisodicLifeEnv(env)\n",
        "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "        env = FireResetEnv(env)\n",
        "    env = WarpFrame(env)\n",
        "    env = ClipRewardEnv(env)\n",
        "    return env\n",
        "\n",
        "def evaluate_agent():\n",
        "    # Create the environment for evaluation\n",
        "    env_id = \"ALE/Tennis-v5\"\n",
        "    env = gym.make(env_id, render_mode='human')\n",
        "\n",
        "    # Apply necessary wrappers\n",
        "\n",
        "    env = MaxAndSkipEnv(env, skip=4)\n",
        "    env = EpisodicLifeEnv(env)\n",
        "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "        env = FireResetEnv(env)\n",
        "    env = WarpFrame(env)\n",
        "    env = ClipRewardEnv(env)\n",
        "\n",
        "    # Stack frames\n",
        "    env = DummyVecEnv([lambda: env])\n",
        "    env = VecFrameStack(env, n_stack=4)\n",
        "\n",
        "    # Load the trained model\n",
        "    model = PPO.load(\"ppo_atari_tennis\")\n",
        "\n",
        "    obs = env.reset()\n",
        "    while True:\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, rewards, dones, infos = env.step(action)\n",
        "        # Rendering is handled by the environment when render_mode='human'\n",
        "        if dones:\n",
        "            obs = env.reset()\n",
        "\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment ID for Atari Tennis\n",
        "env_id = \"ALE/Tennis-v5\"\n",
        "\n",
        "# Number of parallel environments (increase for faster training)\n",
        "num_envs = 8  # You can adjust this number\n",
        "\n",
        "# Create the vectorized environment\n",
        "env = make_env(env_id)\n",
        "env = DummyVecEnv([lambda: env for _ in range(num_envs)])\n",
        "\n",
        "# Stack frames (for temporal information)\n",
        "env = VecFrameStack(env, n_stack=4)\n",
        "\n",
        "# Create the PPO agent with CNN policy (since observations are images)\n",
        "model = PPO(\"CnnPolicy\", env, verbose=1)\n",
        "\n",
        "# Train the agent\n",
        "total_timesteps = 10_000_000  # Adjust as needed\n",
        "model.learn(total_timesteps=total_timesteps)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"ppo_atari_tennis\")\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n",
        "\n",
        "# Evaluate the trained agent\n",
        "evaluate_agent()"
      ],
      "metadata": {
        "id": "haCywSvPiY3o",
        "outputId": "824167e4-ac02-4108-d471-2a400dde8e34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ParallelAtariEnv' object has no attribute 'get_action_meanings'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-92fd8611531c>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create the vectorized environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-bc85f5baf8b0>\u001b[0m in \u001b[0;36mmake_env\u001b[0;34m(env_id)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#env = gym.make(env_id, render_mode='rgb_array')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNoopResetEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoop_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxAndSkipEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEpisodicLifeEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/atari_wrappers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, noop_max)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverride_num_noops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoop_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action_meanings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"NOOP\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAtariResetReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ParallelAtariEnv' object has no attribute 'get_action_meanings'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pettingzoo.atari import tennis_v3\n",
        "\n",
        "#Environments can be interacted with in a manner very similar to Gymnasium:\n",
        "\n",
        "env.reset()\n",
        "for agent in env.agent_iter():\n",
        "    observation, reward, termination, truncation, info = env.last()\n",
        "    action = None if termination or truncation else env.action_space(agent).sample()  # this is where you would insert your policy\n",
        "    env.step(action)"
      ],
      "metadata": {
        "id": "5Jd9uSXxxeYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = tennis_v3.env()\n",
        "env = NoopResetEnv(env, noop_max=30)"
      ],
      "metadata": {
        "id": "_CjuQ0tCy5AU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Uses Stable-Baselines3 to train agents in the Knights-Archers-Zombies environment using SuperSuit vector envs.\n",
        "\n",
        "This environment requires using SuperSuit's Black Death wrapper, to handle agent death.\n",
        "\n",
        "For more information, see https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html\n",
        "\n",
        "Author: Elliot (https://github.com/elliottower)\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "\n",
        "import supersuit as ss\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.ppo import CnnPolicy, MlpPolicy\n",
        "\n",
        "from pettingzoo.butterfly import knights_archers_zombies_v10\n",
        "\n",
        "\n",
        "def train(env_fn, steps: int = 10_000, seed: int | None = 0, **env_kwargs):\n",
        "    # Train a single model to play as each agent in an AEC environment\n",
        "    env = env_fn.parallel_env(**env_kwargs)\n",
        "\n",
        "    # Add black death wrapper so the number of agents stays constant\n",
        "    # MarkovVectorEnv does not support environments with varying numbers of active agents unless black_death is set to True\n",
        "    env = ss.black_death_v3(env)\n",
        "\n",
        "    # Pre-process using SuperSuit\n",
        "    visual_observation = not env.unwrapped.vector_state\n",
        "    if visual_observation:\n",
        "        # If the observation space is visual, reduce the color channels, resize from 512px to 84px, and apply frame stacking\n",
        "        env = ss.color_reduction_v0(env, mode=\"B\")\n",
        "        env = ss.resize_v1(env, x_size=84, y_size=84)\n",
        "        env = ss.frame_stack_v1(env, 3)\n",
        "\n",
        "    env.reset(seed=seed)\n",
        "\n",
        "    print(f\"Starting training on {str(env.metadata['name'])}.\")\n",
        "\n",
        "    env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
        "    env = ss.concat_vec_envs_v1(env, 8, num_cpus=1, base_class=\"stable_baselines3\")\n",
        "\n",
        "    # Use a CNN policy if the observation space is visual\n",
        "    model = PPO(\n",
        "        CnnPolicy if visual_observation else MlpPolicy,\n",
        "        env,\n",
        "        verbose=3,\n",
        "        batch_size=256,\n",
        "    )\n",
        "\n",
        "    model.learn(total_timesteps=steps)\n",
        "\n",
        "    model.save(f\"{env.unwrapped.metadata.get('name')}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
        "\n",
        "    print(\"Model has been saved.\")\n",
        "\n",
        "    print(f\"Finished training on {str(env.unwrapped.metadata['name'])}.\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "\n",
        "def eval(env_fn, num_games: int = 100, render_mode: str | None = None, **env_kwargs):\n",
        "    # Evaluate a trained agent vs a random agent\n",
        "    env = env_fn.env(render_mode=render_mode, **env_kwargs)\n",
        "\n",
        "    # Pre-process using SuperSuit\n",
        "    visual_observation = not env.unwrapped.vector_state\n",
        "    if visual_observation:\n",
        "        # If the observation space is visual, reduce the color channels, resize from 512px to 84px, and apply frame stacking\n",
        "        env = ss.color_reduction_v0(env, mode=\"B\")\n",
        "        env = ss.resize_v1(env, x_size=84, y_size=84)\n",
        "        env = ss.frame_stack_v1(env, 3)\n",
        "\n",
        "    print(\n",
        "        f\"\\nStarting evaluation on {str(env.metadata['name'])} (num_games={num_games}, render_mode={render_mode})\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        latest_policy = max(\n",
        "            glob.glob(f\"{env.metadata['name']}*.zip\"), key=os.path.getctime\n",
        "        )\n",
        "    except ValueError:\n",
        "        print(\"Policy not found.\")\n",
        "        exit(0)\n",
        "\n",
        "    model = PPO.load(latest_policy)\n",
        "\n",
        "    rewards = {agent: 0 for agent in env.possible_agents}\n",
        "\n",
        "    # Note: we evaluate here using an AEC environments, to allow for easy A/B testing against random policies\n",
        "    # For example, we can see here that using a random agent for archer_0 results in less points than the trained agent\n",
        "    for i in range(num_games):\n",
        "        env.reset(seed=i)\n",
        "        env.action_space(env.possible_agents[0]).seed(i)\n",
        "\n",
        "        for agent in env.agent_iter():\n",
        "            obs, reward, termination, truncation, info = env.last()\n",
        "\n",
        "            for a in env.agents:\n",
        "                rewards[a] += env.rewards[a]\n",
        "\n",
        "            if termination or truncation:\n",
        "                break\n",
        "            else:\n",
        "                if agent == env.possible_agents[0]:\n",
        "                    act = env.action_space(agent).sample()\n",
        "                else:\n",
        "                    act = model.predict(obs, deterministic=True)[0]\n",
        "            env.step(act)\n",
        "    env.close()\n",
        "\n",
        "    avg_reward = sum(rewards.values()) / len(rewards.values())\n",
        "    avg_reward_per_agent = {\n",
        "        agent: rewards[agent] / num_games for agent in env.possible_agents\n",
        "    }\n",
        "    print(f\"Avg reward: {avg_reward}\")\n",
        "    print(\"Avg reward per agent, per game: \", avg_reward_per_agent)\n",
        "    print(\"Full rewards: \", rewards)\n",
        "    return avg_reward\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env_fn = knights_archers_zombies_v10\n",
        "\n",
        "    # Set vector_state to false in order to use visual observations (significantly longer training time)\n",
        "    env_kwargs = dict(max_cycles=100, max_zombies=4, vector_state=True)\n",
        "\n",
        "    # Train a model (takes ~5 minutes on a laptop CPU)\n",
        "    train(env_fn, steps=81_920, seed=0, **env_kwargs)\n",
        "\n",
        "    # Evaluate 10 games (takes ~10 seconds on a laptop CPU)\n",
        "    eval(env_fn, num_games=10, render_mode=None, **env_kwargs)\n",
        "\n",
        "    # Watch 2 games (takes ~10 seconds on a laptop CPU)\n",
        "    eval(env_fn, num_games=2, render_mode=\"human\", **env_kwargs)"
      ],
      "metadata": {
        "id": "QTY7yBHNzsJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import supersuit as ss\n",
        "from pettingzoo.atari import tennis_v3\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import VecMonitor\n",
        "from pettingzoo.utils.conversions import aec_to_parallel\n",
        "\n",
        "# Create the PettingZoo environment\n",
        "env = tennis_v3.env()\n",
        "\n",
        "# Apply Supersuit wrappers to make the environment compatible with Stable Baselines3\n",
        "env = ss.max_observation_v0(env, 2)  # Ensure observations are the same size\n",
        "env = ss.pad_action_space_v0(env)     # Ensure action spaces are the same size\n",
        "env = aec_to_parallel(env)\n",
        "#env = ss.pettingzoo_env_to_vec_env_v1(env)  # Convert to vectorized environment\n",
        "#env = VecMonitor(env)  # Monitor to keep track of rewards and other info\n",
        "\n",
        "# Create the model using the CNN policy for processing image observations\n",
        "model = PPO('CnnPolicy', env, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=500000)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"ppo_tennis_selfplay\")\n",
        "\n",
        "# Load the trained model\n",
        "model = PPO.load(\"ppo_tennis_selfplay\")\n",
        "\n",
        "# Evaluation loop\n",
        "env = tennis_v3.env()\n",
        "env = ss.max_observation_v0(env, 2)\n",
        "env = ss.pad_action_space_v0(env)\n",
        "env.reset()\n",
        "\n",
        "for agent in env.agent_iter():\n",
        "    observation, reward, done, info = env.last()\n",
        "    if done:\n",
        "        action = None\n",
        "    else:\n",
        "        # Use the trained model to predict actions\n",
        "        action, _ = model.predict(observation, deterministic=True)\n",
        "    env.step(action)\n",
        "    env.render()"
      ],
      "metadata": {
        "id": "LH1FW53I0yil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.env import PettingZooEnv\n",
        "from pettingzoo.atari import tennis_v3\n",
        "from ray.tune.registry import register_env\n",
        "from ray.rllib.agents.ppo import PPOTrainer\n",
        "\n",
        "# Initialize Ray\n",
        "ray.init()\n",
        "\n",
        "# Environment creator function\n",
        "def env_creator(config):\n",
        "    env = tennis_v3.env()\n",
        "    return env\n",
        "\n",
        "# Register the environment with RLlib\n",
        "register_env(\"tennis_v3\", lambda config: PettingZooEnv(env_creator(config)))\n",
        "\n",
        "# Create an instance of the environment to extract spaces\n",
        "temp_env = PettingZooEnv(env_creator({}))\n",
        "obs_space = temp_env.observation_space\n",
        "act_space = temp_env.action_space\n",
        "\n",
        "# Define the policies\n",
        "policies = {\n",
        "    \"shared_policy\": (None, obs_space, act_space, {})\n",
        "}\n",
        "\n",
        "# Policy mapping function\n",
        "def policy_mapping_fn(agent_id, episode, **kwargs):\n",
        "    return \"shared_policy\"  # All agents use the same policy (self-play)\n",
        "\n",
        "# RLlib configuration\n",
        "config = {\n",
        "    \"env\": \"tennis_v3\",\n",
        "    \"env_config\": {},\n",
        "    \"framework\": \"torch\",  # Use \"tf\" if you prefer TensorFlow\n",
        "    \"num_workers\": 1,      # Increase if you have more CPUs\n",
        "    \"num_gpus\": 0,         # Set to 1 if you have a GPU\n",
        "    \"multiagent\": {\n",
        "        \"policies\": policies,\n",
        "        \"policy_mapping_fn\": policy_mapping_fn,\n",
        "    },\n",
        "    \"lr\": 1e-4,\n",
        "    \"train_batch_size\": 4000,\n",
        "    \"rollout_fragment_length\": 200,\n",
        "    \"sgd_minibatch_size\": 128,\n",
        "    \"num_sgd_iter\": 10,\n",
        "    \"clip_param\": 0.1,\n",
        "}\n",
        "\n",
        "# Start training\n",
        "results = tune.run(\n",
        "    \"PPO\",\n",
        "    config=config,\n",
        "    stop={\"timesteps_total\": 500000},\n",
        "    checkpoint_at_end=True,\n",
        ")\n",
        "\n",
        "# Get the last checkpoint\n",
        "checkpoints = results.get_trial_checkpoints_paths(\n",
        "    trial=results.get_best_trial(\"episode_reward_mean\", mode=\"max\"),\n",
        "    metric=\"episode_reward_mean\"\n",
        ")\n",
        "checkpoint_path = checkpoints[0][0]\n",
        "\n",
        "# Load the trained agent\n",
        "agent = PPOTrainer(config=config)\n",
        "agent.restore(checkpoint_path)\n",
        "\n",
        "# Evaluation loop\n",
        "env = PettingZooEnv(env_creator({}))\n",
        "env.reset()\n",
        "\n",
        "for agent_id in env.agent_iter():\n",
        "    observation, reward, done, info = env.last()\n",
        "    if done:\n",
        "        action = None\n",
        "    else:\n",
        "        action = agent.compute_single_action(observation, policy_id=\"shared_policy\")\n",
        "    env.step(action)\n",
        "    env.render()\n",
        "\n",
        "# Shutdown Ray\n",
        "ray.shutdown()\n"
      ],
      "metadata": {
        "id": "EL6yKRU6iy7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.env import PettingZooEnv\n",
        "from pettingzoo.atari import tennis_v3\n",
        "from ray.tune.registry import register_env\n",
        "from ray.rllib.algoirthms.ppo import PPOTrainer\n",
        "\n",
        "# Initialize Ray\n",
        "ray.init()\n",
        "\n",
        "# Environment creator function\n",
        "def env_creator(config):\n",
        "    env = tennis_v3.env()\n",
        "    return env\n",
        "\n",
        "# Register the environment with RLlib\n",
        "register_env(\"tennis_v3\", lambda config: PettingZooEnv(env_creator(config)))\n",
        "\n",
        "# Create an instance of the environment to extract spaces\n",
        "temp_env = PettingZooEnv(env_creator({}))\n",
        "obs_space = temp_env.observation_space\n",
        "act_space = temp_env.action_space\n",
        "\n",
        "# Define the policies\n",
        "policies = {\n",
        "    \"shared_policy\": (None, obs_space, act_space, {})\n",
        "}\n",
        "\n",
        "# Policy mapping function\n",
        "def policy_mapping_fn(agent_id, episode, **kwargs):\n",
        "    return \"shared_policy\"  # All agents use the same policy (self-play)\n",
        "\n",
        "# RLlib configuration\n",
        "config = {\n",
        "    \"env\": \"tennis_v3\",\n",
        "    \"env_config\": {},\n",
        "    \"framework\": \"torch\",  # Use \"tf\" if you prefer TensorFlow\n",
        "    \"num_workers\": 1,      # Increase if you have more CPUs\n",
        "    \"num_gpus\": 0,         # Set to 1 if you have a GPU\n",
        "    \"multiagent\": {\n",
        "        \"policies\": policies,\n",
        "        \"policy_mapping_fn\": policy_mapping_fn,\n",
        "    },\n",
        "    \"lr\": 1e-4,\n",
        "    \"train_batch_size\": 4000,\n",
        "    \"rollout_fragment_length\": 200,\n",
        "    \"sgd_minibatch_size\": 128,\n",
        "    \"num_sgd_iter\": 10,\n",
        "    \"clip_param\": 0.1,\n",
        "}\n",
        "\n",
        "# Start training\n",
        "results = tune.run(\n",
        "    \"PPO\",\n",
        "    config=config,\n",
        "    stop={\"timesteps_total\": 500000},\n",
        "    checkpoint_at_end=True,\n",
        ")\n",
        "\n",
        "# Get the last checkpoint\n",
        "checkpoints = results.get_trial_checkpoints_paths(\n",
        "    trial=results.get_best_trial(\"episode_reward_mean\", mode=\"max\"),\n",
        "    metric=\"episode_reward_mean\"\n",
        ")\n",
        "checkpoint_path = checkpoints[0][0]\n",
        "\n",
        "# Load the trained agent\n",
        "agent = PPOTrainer(config=config)\n",
        "agent.restore(checkpoint_path)\n",
        "\n",
        "# Evaluation loop\n",
        "env = PettingZooEnv(env_creator({}))\n",
        "env.reset()\n",
        "\n",
        "for agent_id in env.agent_iter():\n",
        "    observation, reward, done, info = env.last()\n",
        "    if done:\n",
        "        action = None\n",
        "    else:\n",
        "        action = agent.compute_single_action(observation, policy_id=\"shared_policy\")\n",
        "    env.step(action)\n",
        "    env.render()\n",
        "\n",
        "# Shutdown Ray\n",
        "ray.shutdown()\n"
      ],
      "metadata": {
        "id": "Nn4YW58rlwts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Uses Ray's RLlib to train agents to play Pistonball.\n",
        "\n",
        "Author: Rohan (https://github.com/Rohan138)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "import ray\n",
        "import supersuit as ss\n",
        "from ray import tune\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
        "from ray.rllib.models import ModelCatalog\n",
        "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
        "from ray.tune.registry import register_env\n",
        "from torch import nn\n",
        "\n",
        "from pettingzoo.butterfly import pistonball_v6\n",
        "\n",
        "\n",
        "class CNNModelV2(TorchModelV2, nn.Module):\n",
        "    def __init__(self, obs_space, act_space, num_outputs, *args, **kwargs):\n",
        "        TorchModelV2.__init__(self, obs_space, act_space, num_outputs, *args, **kwargs)\n",
        "        nn.Module.__init__(self)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, [8, 8], stride=(4, 4)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, [4, 4], stride=(2, 2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, [3, 3], stride=(1, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            (nn.Linear(3136, 512)),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.policy_fn = nn.Linear(512, num_outputs)\n",
        "        self.value_fn = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, input_dict, state, seq_lens):\n",
        "        model_out = self.model(input_dict[\"obs\"].permute(0, 3, 1, 2))\n",
        "        self._value_out = self.value_fn(model_out)\n",
        "        return self.policy_fn(model_out), state\n",
        "\n",
        "    def value_function(self):\n",
        "        return self._value_out.flatten()\n",
        "\n",
        "\n",
        "def env_creator(args):\n",
        "    env = pistonball_v6.parallel_env(\n",
        "        n_pistons=20,\n",
        "        time_penalty=-0.1,\n",
        "        continuous=True,\n",
        "        random_drop=True,\n",
        "        random_rotate=True,\n",
        "        ball_mass=0.75,\n",
        "        ball_friction=0.3,\n",
        "        ball_elasticity=1.5,\n",
        "        max_cycles=125,\n",
        "    )\n",
        "    env = ss.color_reduction_v0(env, mode=\"B\")\n",
        "    env = ss.dtype_v0(env, \"float32\")\n",
        "    env = ss.resize_v1(env, x_size=84, y_size=84)\n",
        "    env = ss.normalize_obs_v0(env, env_min=0, env_max=1)\n",
        "    env = ss.frame_stack_v1(env, 3)\n",
        "    return env\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     ray.init()\n",
        "\n",
        "#     env_name = \"pistonball_v6\"\n",
        "\n",
        "#     register_env(env_name, lambda config: ParallelPettingZooEnv(env_creator(config)))\n",
        "#     ModelCatalog.register_custom_model(\"CNNModelV2\", CNNModelV2)\n",
        "\n",
        "#     config = (\n",
        "#         PPOConfig()\n",
        "#         .environment(env=env_name, clip_actions=True)\n",
        "#         .rollouts(num_rollout_workers=4, rollout_fragment_length=128)\n",
        "#         .training(\n",
        "#             train_batch_size=512,\n",
        "#             lr=2e-5,\n",
        "#             gamma=0.99,\n",
        "#             lambda_=0.9,\n",
        "#             use_gae=True,\n",
        "#             clip_param=0.4,\n",
        "#             grad_clip=None,\n",
        "#             entropy_coeff=0.1,\n",
        "#             vf_loss_coeff=0.25,\n",
        "#             sgd_minibatch_size=64,\n",
        "#             num_sgd_iter=10,\n",
        "#         )\n",
        "#         .debugging(log_level=\"ERROR\")\n",
        "#         .framework(framework=\"torch\")\n",
        "#         .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
        "#     )\n",
        "\n",
        "#     tune.run(\n",
        "#         \"PPO\",\n",
        "#         name=\"PPO\",\n",
        "#         stop={\"timesteps_total\": 5000000 if not os.environ.get(\"CI\") else 50000},\n",
        "#         checkpoint_freq=10,\n",
        "#         storage_path=\"~/ray_results/\" + env_name,\n",
        "#         config=config.to_dict(),\n",
        "#     )"
      ],
      "metadata": {
        "id": "fsA0mDWpmO63",
        "outputId": "c7cef102-47a4-4f4d-b4d7-973ca1936c31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ray.init()\n",
        "\n",
        "env_name = \"pistonball_v6\"\n",
        "\n",
        "register_env(env_name, lambda config: ParallelPettingZooEnv(env_creator(config)))\n",
        "ModelCatalog.register_custom_model(\"CNNModelV2\", CNNModelV2)\n",
        "\n",
        "config = (\n",
        "    PPOConfig()\n",
        "    .environment(env=env_name, clip_actions=True)\n",
        "    .rollouts(num_rollout_workers=4, rollout_fragment_length=128)\n",
        "    .training(\n",
        "        train_batch_size=512,\n",
        "        lr=2e-5,\n",
        "        gamma=0.99,\n",
        "        lambda_=0.9,\n",
        "        use_gae=True,\n",
        "        clip_param=0.4,\n",
        "        grad_clip=None,\n",
        "        entropy_coeff=0.1,\n",
        "        vf_loss_coeff=0.25,\n",
        "        sgd_minibatch_size=64,\n",
        "        num_sgd_iter=10,\n",
        "    )\n",
        "    .debugging(log_level=\"ERROR\")\n",
        "    .framework(framework=\"torch\")\n",
        "    .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
        ")\n",
        "\n",
        "tune.run(\n",
        "    \"PPO\",\n",
        "    name=\"PPO\",\n",
        "    stop={\"timesteps_total\": 5000000 if not os.environ.get(\"CI\") else 50000},\n",
        "    checkpoint_freq=10,\n",
        "    storage_path=\"~/ray_results/\" + env_name,\n",
        "    config=config.to_dict(),\n",
        ")"
      ],
      "metadata": {
        "id": "ZH-ei03GnEtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/ray-project/ray/issues/16425"
      ],
      "metadata": {
        "id": "CFo4Ls-WuT57"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}