{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOnKDqOEKDcDFPJpAOp2o7G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuds/rl-atari-tennis/blob/main/%5BAtari%20Tennis%5D%20Reinforcement%20Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Atari Tennis"
      ],
      "metadata": {
        "id": "ob-2hGT3jNdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig"
      ],
      "metadata": {
        "id": "Fy1WJ1R-iP4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium gymnasium[atari] pettingzoo multi-agent-ale-py autorom"
      ],
      "metadata": {
        "id": "7U_qTvGlj2oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!AutoROM --accept-license"
      ],
      "metadata": {
        "id": "Le7dztPUkVz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ray[rllib] pymunk"
      ],
      "metadata": {
        "id": "iwt4C2URiRGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supersuit stable-baselines3"
      ],
      "metadata": {
        "id": "NmOqw4UAmAoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import platform\n",
        "import ray\n",
        "import supersuit\n",
        "import torch\n",
        "import numpy\n",
        "import gymnasium as gym\n",
        "from pettingzoo.atari import tennis_v3\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.atari_wrappers import (\n",
        "    NoopResetEnv, MaxAndSkipEnv, EpisodicLifeEnv,\n",
        "    FireResetEnv, WarpFrame, ClipRewardEnv\n",
        ")\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
        "from importlib.metadata import version\n",
        "import time"
      ],
      "metadata": {
        "id": "o89JlEiexh1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Python Version: {platform.python_version()}\")\n",
        "print(f\"Torch Version: {version('torch')}\")\n",
        "print(f\"Is Cuda Available: {torch.cuda.is_available()}\")\n",
        "print(f\"Cuda Version: {torch.version.cuda}\")\n",
        "print(f\"Gymnasium Version: {version('gymnasium')}\")\n",
        "print(f\"Numpy Version: {version('numpy')}\")\n",
        "print(f\"Stable Baselines3 Version: {version('stable_baselines3')}\")\n",
        "print(f\"Supersuit Version: {version('supersuit')}\")\n",
        "print(f\"PettingZoo Version: {version('pettingzoo')}\")\n",
        "print(f\"Ray Version: {version('ray')}\")"
      ],
      "metadata": {
        "id": "KRrJSPuDiDN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvPXWTCHiO69"
      },
      "outputs": [],
      "source": [
        "def make_env(env_id):\n",
        "    \"\"\"\n",
        "    Creates and wraps the Atari environment.\n",
        "    \"\"\"\n",
        "    env = tennis_v3.env(render_mode=\"rgb_array\")\n",
        "    env.reset(seed=42)\n",
        "    #env = gym.make(env_id, render_mode='rgb_array')\n",
        "    env = NoopResetEnv(env, noop_max=30)\n",
        "    env = MaxAndSkipEnv(env, skip=4)\n",
        "    env = EpisodicLifeEnv(env)\n",
        "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "        env = FireResetEnv(env)\n",
        "    env = WarpFrame(env)\n",
        "    env = ClipRewardEnv(env)\n",
        "    return env\n",
        "\n",
        "def evaluate_agent():\n",
        "    # Create the environment for evaluation\n",
        "    env_id = \"ALE/Tennis-v5\"\n",
        "    env = gym.make(env_id, render_mode='human')\n",
        "\n",
        "    # Apply necessary wrappers\n",
        "\n",
        "    env = MaxAndSkipEnv(env, skip=4)\n",
        "    env = EpisodicLifeEnv(env)\n",
        "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "        env = FireResetEnv(env)\n",
        "    env = WarpFrame(env)\n",
        "    env = ClipRewardEnv(env)\n",
        "\n",
        "    # Stack frames\n",
        "    env = DummyVecEnv([lambda: env])\n",
        "    env = VecFrameStack(env, n_stack=4)\n",
        "\n",
        "    # Load the trained model\n",
        "    model = PPO.load(\"ppo_atari_tennis\")\n",
        "\n",
        "    obs = env.reset()\n",
        "    while True:\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, rewards, dones, infos = env.step(action)\n",
        "        # Rendering is handled by the environment when render_mode='human'\n",
        "        if dones:\n",
        "            obs = env.reset()\n",
        "\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment ID for Atari Tennis\n",
        "env_id = \"ALE/Tennis-v5\"\n",
        "\n",
        "# Number of parallel environments (increase for faster training)\n",
        "num_envs = 8  # You can adjust this number\n",
        "\n",
        "# Create the vectorized environment\n",
        "env = make_env(env_id)\n",
        "env = DummyVecEnv([lambda: env for _ in range(num_envs)])\n",
        "\n",
        "# Stack frames (for temporal information)\n",
        "env = VecFrameStack(env, n_stack=4)\n",
        "\n",
        "# Create the PPO agent with CNN policy (since observations are images)\n",
        "model = PPO(\"CnnPolicy\", env, verbose=1)\n",
        "\n",
        "# Train the agent\n",
        "total_timesteps = 10_000_000  # Adjust as needed\n",
        "model.learn(total_timesteps=total_timesteps)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"ppo_atari_tennis\")\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n",
        "\n",
        "# Evaluate the trained agent\n",
        "evaluate_agent()"
      ],
      "metadata": {
        "id": "haCywSvPiY3o",
        "outputId": "824167e4-ac02-4108-d471-2a400dde8e34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ParallelAtariEnv' object has no attribute 'get_action_meanings'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-92fd8611531c>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create the vectorized environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-bc85f5baf8b0>\u001b[0m in \u001b[0;36mmake_env\u001b[0;34m(env_id)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#env = gym.make(env_id, render_mode='rgb_array')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNoopResetEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoop_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxAndSkipEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEpisodicLifeEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/atari_wrappers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, noop_max)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverride_num_noops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoop_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action_meanings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"NOOP\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAtariResetReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ParallelAtariEnv' object has no attribute 'get_action_meanings'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pettingzoo.atari import tennis_v3\n",
        "\n",
        "#Environments can be interacted with in a manner very similar to Gymnasium:\n",
        "\n",
        "env.reset()\n",
        "for agent in env.agent_iter():\n",
        "    observation, reward, termination, truncation, info = env.last()\n",
        "    action = None if termination or truncation else env.action_space(agent).sample()  # this is where you would insert your policy\n",
        "    env.step(action)"
      ],
      "metadata": {
        "id": "5Jd9uSXxxeYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = tennis_v3.env()\n",
        "env = NoopResetEnv(env, noop_max=30)"
      ],
      "metadata": {
        "id": "_CjuQ0tCy5AU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Uses Stable-Baselines3 to train agents in the Knights-Archers-Zombies environment using SuperSuit vector envs.\n",
        "\n",
        "This environment requires using SuperSuit's Black Death wrapper, to handle agent death.\n",
        "\n",
        "For more information, see https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html\n",
        "\n",
        "Author: Elliot (https://github.com/elliottower)\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "\n",
        "import supersuit as ss\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.ppo import CnnPolicy, MlpPolicy\n",
        "\n",
        "from pettingzoo.butterfly import knights_archers_zombies_v10\n",
        "\n",
        "\n",
        "def train(env_fn, steps: int = 10_000, seed: int | None = 0, **env_kwargs):\n",
        "    # Train a single model to play as each agent in an AEC environment\n",
        "    env = env_fn.parallel_env(**env_kwargs)\n",
        "\n",
        "    # Add black death wrapper so the number of agents stays constant\n",
        "    # MarkovVectorEnv does not support environments with varying numbers of active agents unless black_death is set to True\n",
        "    env = ss.black_death_v3(env)\n",
        "\n",
        "    # Pre-process using SuperSuit\n",
        "    visual_observation = not env.unwrapped.vector_state\n",
        "    if visual_observation:\n",
        "        # If the observation space is visual, reduce the color channels, resize from 512px to 84px, and apply frame stacking\n",
        "        env = ss.color_reduction_v0(env, mode=\"B\")\n",
        "        env = ss.resize_v1(env, x_size=84, y_size=84)\n",
        "        env = ss.frame_stack_v1(env, 3)\n",
        "\n",
        "    env.reset(seed=seed)\n",
        "\n",
        "    print(f\"Starting training on {str(env.metadata['name'])}.\")\n",
        "\n",
        "    env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
        "    env = ss.concat_vec_envs_v1(env, 8, num_cpus=1, base_class=\"stable_baselines3\")\n",
        "\n",
        "    # Use a CNN policy if the observation space is visual\n",
        "    model = PPO(\n",
        "        CnnPolicy if visual_observation else MlpPolicy,\n",
        "        env,\n",
        "        verbose=3,\n",
        "        batch_size=256,\n",
        "    )\n",
        "\n",
        "    model.learn(total_timesteps=steps)\n",
        "\n",
        "    model.save(f\"{env.unwrapped.metadata.get('name')}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
        "\n",
        "    print(\"Model has been saved.\")\n",
        "\n",
        "    print(f\"Finished training on {str(env.unwrapped.metadata['name'])}.\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "\n",
        "def eval(env_fn, num_games: int = 100, render_mode: str | None = None, **env_kwargs):\n",
        "    # Evaluate a trained agent vs a random agent\n",
        "    env = env_fn.env(render_mode=render_mode, **env_kwargs)\n",
        "\n",
        "    # Pre-process using SuperSuit\n",
        "    visual_observation = not env.unwrapped.vector_state\n",
        "    if visual_observation:\n",
        "        # If the observation space is visual, reduce the color channels, resize from 512px to 84px, and apply frame stacking\n",
        "        env = ss.color_reduction_v0(env, mode=\"B\")\n",
        "        env = ss.resize_v1(env, x_size=84, y_size=84)\n",
        "        env = ss.frame_stack_v1(env, 3)\n",
        "\n",
        "    print(\n",
        "        f\"\\nStarting evaluation on {str(env.metadata['name'])} (num_games={num_games}, render_mode={render_mode})\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        latest_policy = max(\n",
        "            glob.glob(f\"{env.metadata['name']}*.zip\"), key=os.path.getctime\n",
        "        )\n",
        "    except ValueError:\n",
        "        print(\"Policy not found.\")\n",
        "        exit(0)\n",
        "\n",
        "    model = PPO.load(latest_policy)\n",
        "\n",
        "    rewards = {agent: 0 for agent in env.possible_agents}\n",
        "\n",
        "    # Note: we evaluate here using an AEC environments, to allow for easy A/B testing against random policies\n",
        "    # For example, we can see here that using a random agent for archer_0 results in less points than the trained agent\n",
        "    for i in range(num_games):\n",
        "        env.reset(seed=i)\n",
        "        env.action_space(env.possible_agents[0]).seed(i)\n",
        "\n",
        "        for agent in env.agent_iter():\n",
        "            obs, reward, termination, truncation, info = env.last()\n",
        "\n",
        "            for a in env.agents:\n",
        "                rewards[a] += env.rewards[a]\n",
        "\n",
        "            if termination or truncation:\n",
        "                break\n",
        "            else:\n",
        "                if agent == env.possible_agents[0]:\n",
        "                    act = env.action_space(agent).sample()\n",
        "                else:\n",
        "                    act = model.predict(obs, deterministic=True)[0]\n",
        "            env.step(act)\n",
        "    env.close()\n",
        "\n",
        "    avg_reward = sum(rewards.values()) / len(rewards.values())\n",
        "    avg_reward_per_agent = {\n",
        "        agent: rewards[agent] / num_games for agent in env.possible_agents\n",
        "    }\n",
        "    print(f\"Avg reward: {avg_reward}\")\n",
        "    print(\"Avg reward per agent, per game: \", avg_reward_per_agent)\n",
        "    print(\"Full rewards: \", rewards)\n",
        "    return avg_reward\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env_fn = knights_archers_zombies_v10\n",
        "\n",
        "    # Set vector_state to false in order to use visual observations (significantly longer training time)\n",
        "    env_kwargs = dict(max_cycles=100, max_zombies=4, vector_state=True)\n",
        "\n",
        "    # Train a model (takes ~5 minutes on a laptop CPU)\n",
        "    train(env_fn, steps=81_920, seed=0, **env_kwargs)\n",
        "\n",
        "    # Evaluate 10 games (takes ~10 seconds on a laptop CPU)\n",
        "    eval(env_fn, num_games=10, render_mode=None, **env_kwargs)\n",
        "\n",
        "    # Watch 2 games (takes ~10 seconds on a laptop CPU)\n",
        "    eval(env_fn, num_games=2, render_mode=\"human\", **env_kwargs)"
      ],
      "metadata": {
        "id": "QTY7yBHNzsJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import supersuit as ss\n",
        "from pettingzoo.atari import tennis_v3\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import VecMonitor\n",
        "from pettingzoo.utils.conversions import aec_to_parallel\n",
        "\n",
        "# Create the PettingZoo environment\n",
        "env = tennis_v3.env()\n",
        "\n",
        "# Apply Supersuit wrappers to make the environment compatible with Stable Baselines3\n",
        "env = ss.max_observation_v0(env, 2)  # Ensure observations are the same size\n",
        "env = ss.pad_action_space_v0(env)     # Ensure action spaces are the same size\n",
        "env = aec_to_parallel(env)\n",
        "#env = ss.pettingzoo_env_to_vec_env_v1(env)  # Convert to vectorized environment\n",
        "#env = VecMonitor(env)  # Monitor to keep track of rewards and other info\n",
        "\n",
        "# Create the model using the CNN policy for processing image observations\n",
        "model = PPO('CnnPolicy', env, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=500000)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"ppo_tennis_selfplay\")\n",
        "\n",
        "# Load the trained model\n",
        "model = PPO.load(\"ppo_tennis_selfplay\")\n",
        "\n",
        "# Evaluation loop\n",
        "env = tennis_v3.env()\n",
        "env = ss.max_observation_v0(env, 2)\n",
        "env = ss.pad_action_space_v0(env)\n",
        "env.reset()\n",
        "\n",
        "for agent in env.agent_iter():\n",
        "    observation, reward, done, info = env.last()\n",
        "    if done:\n",
        "        action = None\n",
        "    else:\n",
        "        # Use the trained model to predict actions\n",
        "        action, _ = model.predict(observation, deterministic=True)\n",
        "    env.step(action)\n",
        "    env.render()"
      ],
      "metadata": {
        "id": "LH1FW53I0yil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.env import PettingZooEnv\n",
        "from pettingzoo.atari import tennis_v3\n",
        "from ray.tune.registry import register_env\n",
        "from ray.rllib.agents.ppo import PPOTrainer\n",
        "\n",
        "# Initialize Ray\n",
        "ray.init()\n",
        "\n",
        "# Environment creator function\n",
        "def env_creator(config):\n",
        "    env = tennis_v3.env()\n",
        "    return env\n",
        "\n",
        "# Register the environment with RLlib\n",
        "register_env(\"tennis_v3\", lambda config: PettingZooEnv(env_creator(config)))\n",
        "\n",
        "# Create an instance of the environment to extract spaces\n",
        "temp_env = PettingZooEnv(env_creator({}))\n",
        "obs_space = temp_env.observation_space\n",
        "act_space = temp_env.action_space\n",
        "\n",
        "# Define the policies\n",
        "policies = {\n",
        "    \"shared_policy\": (None, obs_space, act_space, {})\n",
        "}\n",
        "\n",
        "# Policy mapping function\n",
        "def policy_mapping_fn(agent_id, episode, **kwargs):\n",
        "    return \"shared_policy\"  # All agents use the same policy (self-play)\n",
        "\n",
        "# RLlib configuration\n",
        "config = {\n",
        "    \"env\": \"tennis_v3\",\n",
        "    \"env_config\": {},\n",
        "    \"framework\": \"torch\",  # Use \"tf\" if you prefer TensorFlow\n",
        "    \"num_workers\": 1,      # Increase if you have more CPUs\n",
        "    \"num_gpus\": 0,         # Set to 1 if you have a GPU\n",
        "    \"multiagent\": {\n",
        "        \"policies\": policies,\n",
        "        \"policy_mapping_fn\": policy_mapping_fn,\n",
        "    },\n",
        "    \"lr\": 1e-4,\n",
        "    \"train_batch_size\": 4000,\n",
        "    \"rollout_fragment_length\": 200,\n",
        "    \"sgd_minibatch_size\": 128,\n",
        "    \"num_sgd_iter\": 10,\n",
        "    \"clip_param\": 0.1,\n",
        "}\n",
        "\n",
        "# Start training\n",
        "results = tune.run(\n",
        "    \"PPO\",\n",
        "    config=config,\n",
        "    stop={\"timesteps_total\": 500000},\n",
        "    checkpoint_at_end=True,\n",
        ")\n",
        "\n",
        "# Get the last checkpoint\n",
        "checkpoints = results.get_trial_checkpoints_paths(\n",
        "    trial=results.get_best_trial(\"episode_reward_mean\", mode=\"max\"),\n",
        "    metric=\"episode_reward_mean\"\n",
        ")\n",
        "checkpoint_path = checkpoints[0][0]\n",
        "\n",
        "# Load the trained agent\n",
        "agent = PPOTrainer(config=config)\n",
        "agent.restore(checkpoint_path)\n",
        "\n",
        "# Evaluation loop\n",
        "env = PettingZooEnv(env_creator({}))\n",
        "env.reset()\n",
        "\n",
        "for agent_id in env.agent_iter():\n",
        "    observation, reward, done, info = env.last()\n",
        "    if done:\n",
        "        action = None\n",
        "    else:\n",
        "        action = agent.compute_single_action(observation, policy_id=\"shared_policy\")\n",
        "    env.step(action)\n",
        "    env.render()\n",
        "\n",
        "# Shutdown Ray\n",
        "ray.shutdown()\n"
      ],
      "metadata": {
        "id": "EL6yKRU6iy7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.env import PettingZooEnv\n",
        "from pettingzoo.atari import tennis_v3\n",
        "from ray.tune.registry import register_env\n",
        "from ray.rllib.algoirthms.ppo import PPOTrainer\n",
        "\n",
        "# Initialize Ray\n",
        "ray.init()\n",
        "\n",
        "# Environment creator function\n",
        "def env_creator(config):\n",
        "    env = tennis_v3.env()\n",
        "    return env\n",
        "\n",
        "# Register the environment with RLlib\n",
        "register_env(\"tennis_v3\", lambda config: PettingZooEnv(env_creator(config)))\n",
        "\n",
        "# Create an instance of the environment to extract spaces\n",
        "temp_env = PettingZooEnv(env_creator({}))\n",
        "obs_space = temp_env.observation_space\n",
        "act_space = temp_env.action_space\n",
        "\n",
        "# Define the policies\n",
        "policies = {\n",
        "    \"shared_policy\": (None, obs_space, act_space, {})\n",
        "}\n",
        "\n",
        "# Policy mapping function\n",
        "def policy_mapping_fn(agent_id, episode, **kwargs):\n",
        "    return \"shared_policy\"  # All agents use the same policy (self-play)\n",
        "\n",
        "# RLlib configuration\n",
        "config = {\n",
        "    \"env\": \"tennis_v3\",\n",
        "    \"env_config\": {},\n",
        "    \"framework\": \"torch\",  # Use \"tf\" if you prefer TensorFlow\n",
        "    \"num_workers\": 1,      # Increase if you have more CPUs\n",
        "    \"num_gpus\": 0,         # Set to 1 if you have a GPU\n",
        "    \"multiagent\": {\n",
        "        \"policies\": policies,\n",
        "        \"policy_mapping_fn\": policy_mapping_fn,\n",
        "    },\n",
        "    \"lr\": 1e-4,\n",
        "    \"train_batch_size\": 4000,\n",
        "    \"rollout_fragment_length\": 200,\n",
        "    \"sgd_minibatch_size\": 128,\n",
        "    \"num_sgd_iter\": 10,\n",
        "    \"clip_param\": 0.1,\n",
        "}\n",
        "\n",
        "# Start training\n",
        "results = tune.run(\n",
        "    \"PPO\",\n",
        "    config=config,\n",
        "    stop={\"timesteps_total\": 500000},\n",
        "    checkpoint_at_end=True,\n",
        ")\n",
        "\n",
        "# Get the last checkpoint\n",
        "checkpoints = results.get_trial_checkpoints_paths(\n",
        "    trial=results.get_best_trial(\"episode_reward_mean\", mode=\"max\"),\n",
        "    metric=\"episode_reward_mean\"\n",
        ")\n",
        "checkpoint_path = checkpoints[0][0]\n",
        "\n",
        "# Load the trained agent\n",
        "agent = PPOTrainer(config=config)\n",
        "agent.restore(checkpoint_path)\n",
        "\n",
        "# Evaluation loop\n",
        "env = PettingZooEnv(env_creator({}))\n",
        "env.reset()\n",
        "\n",
        "for agent_id in env.agent_iter():\n",
        "    observation, reward, done, info = env.last()\n",
        "    if done:\n",
        "        action = None\n",
        "    else:\n",
        "        action = agent.compute_single_action(observation, policy_id=\"shared_policy\")\n",
        "    env.step(action)\n",
        "    env.render()\n",
        "\n",
        "# Shutdown Ray\n",
        "ray.shutdown()\n"
      ],
      "metadata": {
        "id": "Nn4YW58rlwts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Uses Ray's RLlib to train agents to play Pistonball.\n",
        "\n",
        "Author: Rohan (https://github.com/Rohan138)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "import ray\n",
        "import supersuit as ss\n",
        "from ray import tune\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
        "from ray.rllib.models import ModelCatalog\n",
        "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
        "from ray.tune.registry import register_env\n",
        "from torch import nn\n",
        "\n",
        "from pettingzoo.butterfly import pistonball_v6\n",
        "\n",
        "\n",
        "class CNNModelV2(TorchModelV2, nn.Module):\n",
        "    def __init__(self, obs_space, act_space, num_outputs, *args, **kwargs):\n",
        "        TorchModelV2.__init__(self, obs_space, act_space, num_outputs, *args, **kwargs)\n",
        "        nn.Module.__init__(self)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, [8, 8], stride=(4, 4)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, [4, 4], stride=(2, 2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, [3, 3], stride=(1, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            (nn.Linear(3136, 512)),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.policy_fn = nn.Linear(512, num_outputs)\n",
        "        self.value_fn = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, input_dict, state, seq_lens):\n",
        "        model_out = self.model(input_dict[\"obs\"].permute(0, 3, 1, 2))\n",
        "        self._value_out = self.value_fn(model_out)\n",
        "        return self.policy_fn(model_out), state\n",
        "\n",
        "    def value_function(self):\n",
        "        return self._value_out.flatten()\n",
        "\n",
        "\n",
        "def env_creator(args):\n",
        "    env = pistonball_v6.parallel_env(\n",
        "        n_pistons=20,\n",
        "        time_penalty=-0.1,\n",
        "        continuous=True,\n",
        "        random_drop=True,\n",
        "        random_rotate=True,\n",
        "        ball_mass=0.75,\n",
        "        ball_friction=0.3,\n",
        "        ball_elasticity=1.5,\n",
        "        max_cycles=125,\n",
        "    )\n",
        "    env = ss.color_reduction_v0(env, mode=\"B\")\n",
        "    env = ss.dtype_v0(env, \"float32\")\n",
        "    env = ss.resize_v1(env, x_size=84, y_size=84)\n",
        "    env = ss.normalize_obs_v0(env, env_min=0, env_max=1)\n",
        "    env = ss.frame_stack_v1(env, 3)\n",
        "    return env\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     ray.init()\n",
        "\n",
        "#     env_name = \"pistonball_v6\"\n",
        "\n",
        "#     register_env(env_name, lambda config: ParallelPettingZooEnv(env_creator(config)))\n",
        "#     ModelCatalog.register_custom_model(\"CNNModelV2\", CNNModelV2)\n",
        "\n",
        "#     config = (\n",
        "#         PPOConfig()\n",
        "#         .environment(env=env_name, clip_actions=True)\n",
        "#         .rollouts(num_rollout_workers=4, rollout_fragment_length=128)\n",
        "#         .training(\n",
        "#             train_batch_size=512,\n",
        "#             lr=2e-5,\n",
        "#             gamma=0.99,\n",
        "#             lambda_=0.9,\n",
        "#             use_gae=True,\n",
        "#             clip_param=0.4,\n",
        "#             grad_clip=None,\n",
        "#             entropy_coeff=0.1,\n",
        "#             vf_loss_coeff=0.25,\n",
        "#             sgd_minibatch_size=64,\n",
        "#             num_sgd_iter=10,\n",
        "#         )\n",
        "#         .debugging(log_level=\"ERROR\")\n",
        "#         .framework(framework=\"torch\")\n",
        "#         .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
        "#     )\n",
        "\n",
        "#     tune.run(\n",
        "#         \"PPO\",\n",
        "#         name=\"PPO\",\n",
        "#         stop={\"timesteps_total\": 5000000 if not os.environ.get(\"CI\") else 50000},\n",
        "#         checkpoint_freq=10,\n",
        "#         storage_path=\"~/ray_results/\" + env_name,\n",
        "#         config=config.to_dict(),\n",
        "#     )"
      ],
      "metadata": {
        "id": "fsA0mDWpmO63",
        "outputId": "c7cef102-47a4-4f4d-b4d7-973ca1936c31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ray.init()\n",
        "\n",
        "env_name = \"pistonball_v6\"\n",
        "\n",
        "register_env(env_name, lambda config: ParallelPettingZooEnv(env_creator(config)))\n",
        "ModelCatalog.register_custom_model(\"CNNModelV2\", CNNModelV2)\n",
        "\n",
        "config = (\n",
        "    PPOConfig()\n",
        "    .environment(env=env_name, clip_actions=True)\n",
        "    .rollouts(num_rollout_workers=4, rollout_fragment_length=128)\n",
        "    .training(\n",
        "        train_batch_size=512,\n",
        "        lr=2e-5,\n",
        "        gamma=0.99,\n",
        "        lambda_=0.9,\n",
        "        use_gae=True,\n",
        "        clip_param=0.4,\n",
        "        grad_clip=None,\n",
        "        entropy_coeff=0.1,\n",
        "        vf_loss_coeff=0.25,\n",
        "        sgd_minibatch_size=64,\n",
        "        num_sgd_iter=10,\n",
        "    )\n",
        "    .debugging(log_level=\"ERROR\")\n",
        "    .framework(framework=\"torch\")\n",
        "    .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
        ")\n",
        "\n",
        "tune.run(\n",
        "    \"PPO\",\n",
        "    name=\"PPO\",\n",
        "    stop={\"timesteps_total\": 5000000 if not os.environ.get(\"CI\") else 50000},\n",
        "    checkpoint_freq=10,\n",
        "    storage_path=\"~/ray_results/\" + env_name,\n",
        "    config=config.to_dict(),\n",
        ")"
      ],
      "metadata": {
        "id": "ZH-ei03GnEtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/ray-project/ray/issues/16425"
      ],
      "metadata": {
        "id": "CFo4Ls-WuT57"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}