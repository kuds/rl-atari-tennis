{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPtQ+fzvPf+btQCNeEs1GlY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuds/rl-atari-tennis/blob/main/%5BAtari%20Tennis%5D%20Reinforcement%20Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Atari Tennis"
      ],
      "metadata": {
        "id": "ob-2hGT3jNdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fy1WJ1R-iP4V",
        "outputId": "0a56fa9c-648e-4720-acb0-e7f91260ce76"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium gymnasium[atari] pettingzoo multi-agent-ale-py autorom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7U_qTvGlj2oy",
        "outputId": "96fa6c16-8c9b-4fdc-953c-d3ae1ea49029"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: pettingzoo in /usr/local/lib/python3.10/dist-packages (1.24.3)\n",
            "Requirement already satisfied: multi-agent-ale-py in /usr/local/lib/python3.10/dist-packages (0.1.11)\n",
            "Requirement already satisfied: autorom in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: shimmy<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (0.2.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom) (2.32.3)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (0.8.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (2024.8.30)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (6.4.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!AutoROM --accept-license"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Le7dztPUkVz4",
        "outputId": "c0f4d395-21db-4512-9da7-39f9e1318d37"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoROM will download the Atari 2600 ROMs.\n",
            "They will be installed to:\n",
            "\t/usr/local/lib/python3.10/dist-packages/AutoROM/roms\n",
            "\t/usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms\n",
            "\n",
            "Existing ROMs will be overwritten.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ray[rllib] pymunk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwt4C2URiRGM",
        "outputId": "584df91b-c4e8-4dab-8fa5-9de0a07dd5ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymunk in /usr/local/lib/python3.10/dist-packages (6.8.1)\n",
            "Requirement already satisfied: ray[rllib] in /usr/local/lib/python3.10/dist-packages (2.35.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (3.16.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (24.1)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (6.0.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (2.1.4)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (14.0.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (2024.6.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.1.8)\n",
            "Requirement already satisfied: gymnasium==0.28.1 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.28.1)\n",
            "Requirement already satisfied: lz4 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (4.3.3)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.23.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.13.1)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.12.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (13.8.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (1.26.4)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (0.0.4)\n",
            "Requirement already satisfied: cffi>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from pymunk) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.15.0->pymunk) (2.22)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (0.20.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[rllib]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[rllib]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[rllib]) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->ray[rllib]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->ray[rllib]) (2.16.1)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (3.3)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (2.34.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (2024.8.30)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (0.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer->ray[rllib]) (1.5.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->ray[rllib]) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->ray[rllib]) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supersuit stable-baselines3"
      ],
      "metadata": {
        "id": "NmOqw4UAmAoV",
        "outputId": "cfd167c7-3388-49bf-cb4c-a9db487ed209",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: supersuit in /usr/local/lib/python3.10/dist-packages (3.9.3)\n",
            "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.10/dist-packages (2.3.2)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from supersuit) (1.26.4)\n",
            "Requirement already satisfied: gymnasium>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from supersuit) (0.28.1)\n",
            "Requirement already satisfied: tinyscaler>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from supersuit) (1.2.8)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.4.0+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.7.1)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2024.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import platform\n",
        "import ray\n",
        "import supersuit\n",
        "import torch\n",
        "import numpy\n",
        "import gymnasium as gym\n",
        "from pettingzoo.atari import tennis_v3\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.atari_wrappers import (\n",
        "    NoopResetEnv, MaxAndSkipEnv, EpisodicLifeEnv,\n",
        "    FireResetEnv, WarpFrame, ClipRewardEnv\n",
        ")\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
        "from importlib.metadata import version\n",
        "import time"
      ],
      "metadata": {
        "id": "o89JlEiexh1-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Python Version: {platform.python_version()}\")\n",
        "print(f\"Torch Version: {version('torch')}\")\n",
        "print(f\"Is Cuda Available: {torch.cuda.is_available()}\")\n",
        "print(f\"Cuda Version: {torch.version.cuda}\")\n",
        "print(f\"Gymnasium Version: {version('gymnasium')}\")\n",
        "print(f\"Numpy Version: {version('numpy')}\")\n",
        "print(f\"Stable Baselines3 Version: {version('stable_baselines3')}\")\n",
        "print(f\"Supersuit Version: {version('supersuit')}\")\n",
        "print(f\"PettingZoo Version: {version('pettingzoo')}\")\n",
        "print(f\"Ray Version: {version('ray')}\")"
      ],
      "metadata": {
        "id": "KRrJSPuDiDN4",
        "outputId": "a17121e6-22b2-428e-9a7c-6b6c538c041d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python Version: 3.10.12\n",
            "Torch Version: 2.4.0+cu121\n",
            "Is Cuda Available: True\n",
            "Cuda Version: 12.1\n",
            "Gymnasium Version: 0.28.1\n",
            "Numpy Version: 1.26.4\n",
            "Stable Baselines3 Version: 2.3.2\n",
            "Supersuit Version: 3.9.3\n",
            "PettingZoo Version: 1.24.3\n",
            "Ray Version: 2.35.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SvPXWTCHiO69",
        "outputId": "eadc6a7d-2ed9-411a-e570-9ff82103fa04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def make_env(env_id):\n",
        "    \"\"\"\n",
        "    Creates and wraps the Atari environment.\n",
        "    \"\"\"\n",
        "    env = tennis_v3.env(render_mode=\"rgb_array\")\n",
        "    env.reset(seed=42)\n",
        "    #env = gym.make(env_id, render_mode='rgb_array')\n",
        "    env = NoopResetEnv(env, noop_max=30)\n",
        "    env = MaxAndSkipEnv(env, skip=4)\n",
        "    env = EpisodicLifeEnv(env)\n",
        "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "        env = FireResetEnv(env)\n",
        "    env = WarpFrame(env)\n",
        "    env = ClipRewardEnv(env)\n",
        "    return env\n",
        "\n",
        "def evaluate_agent():\n",
        "    # Create the environment for evaluation\n",
        "    env_id = \"ALE/Tennis-v5\"\n",
        "    env = gym.make(env_id, render_mode='human')\n",
        "\n",
        "    # Apply necessary wrappers\n",
        "\n",
        "    env = MaxAndSkipEnv(env, skip=4)\n",
        "    env = EpisodicLifeEnv(env)\n",
        "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "        env = FireResetEnv(env)\n",
        "    env = WarpFrame(env)\n",
        "    env = ClipRewardEnv(env)\n",
        "\n",
        "    # Stack frames\n",
        "    env = DummyVecEnv([lambda: env])\n",
        "    env = VecFrameStack(env, n_stack=4)\n",
        "\n",
        "    # Load the trained model\n",
        "    model = PPO.load(\"ppo_atari_tennis\")\n",
        "\n",
        "    obs = env.reset()\n",
        "    while True:\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, rewards, dones, infos = env.step(action)\n",
        "        # Rendering is handled by the environment when render_mode='human'\n",
        "        if dones:\n",
        "            obs = env.reset()\n",
        "\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment ID for Atari Tennis\n",
        "env_id = \"ALE/Tennis-v5\"\n",
        "\n",
        "# Number of parallel environments (increase for faster training)\n",
        "num_envs = 8  # You can adjust this number\n",
        "\n",
        "# Create the vectorized environment\n",
        "env = make_env(env_id)\n",
        "env = DummyVecEnv([lambda: env for _ in range(num_envs)])\n",
        "\n",
        "# Stack frames (for temporal information)\n",
        "env = VecFrameStack(env, n_stack=4)\n",
        "\n",
        "# Create the PPO agent with CNN policy (since observations are images)\n",
        "model = PPO(\"CnnPolicy\", env, verbose=1)\n",
        "\n",
        "# Train the agent\n",
        "total_timesteps = 10_000_000  # Adjust as needed\n",
        "model.learn(total_timesteps=total_timesteps)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"ppo_atari_tennis\")\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n",
        "\n",
        "# Evaluate the trained agent\n",
        "evaluate_agent()"
      ],
      "metadata": {
        "id": "haCywSvPiY3o",
        "outputId": "824167e4-ac02-4108-d471-2a400dde8e34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ParallelAtariEnv' object has no attribute 'get_action_meanings'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-92fd8611531c>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create the vectorized environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-bc85f5baf8b0>\u001b[0m in \u001b[0;36mmake_env\u001b[0;34m(env_id)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#env = gym.make(env_id, render_mode='rgb_array')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNoopResetEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoop_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxAndSkipEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEpisodicLifeEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/atari_wrappers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, noop_max)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverride_num_noops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoop_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action_meanings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"NOOP\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAtariResetReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ParallelAtariEnv' object has no attribute 'get_action_meanings'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pettingzoo.atari import tennis_v3\n",
        "\n",
        "#Environments can be interacted with in a manner very similar to Gymnasium:\n",
        "\n",
        "env.reset()\n",
        "for agent in env.agent_iter():\n",
        "    observation, reward, termination, truncation, info = env.last()\n",
        "    action = None if termination or truncation else env.action_space(agent).sample()  # this is where you would insert your policy\n",
        "    env.step(action)"
      ],
      "metadata": {
        "id": "5Jd9uSXxxeYY",
        "outputId": "5a4f6d44-ac13-407c-df4e-4fe1f690ef63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = tennis_v3.env()\n",
        "env = NoopResetEnv(env, noop_max=30)"
      ],
      "metadata": {
        "id": "_CjuQ0tCy5AU",
        "outputId": "dc23e4dc-41c4-448a-c5a6-c3a183701469",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ParallelAtariEnv' object has no attribute 'get_action_meanings'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-bc1a32b3086d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtennis_v3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNoopResetEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoop_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/atari_wrappers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, noop_max)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverride_num_noops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoop_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action_meanings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"NOOP\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAtariResetReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ParallelAtariEnv' object has no attribute 'get_action_meanings'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Uses Stable-Baselines3 to train agents in the Knights-Archers-Zombies environment using SuperSuit vector envs.\n",
        "\n",
        "This environment requires using SuperSuit's Black Death wrapper, to handle agent death.\n",
        "\n",
        "For more information, see https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html\n",
        "\n",
        "Author: Elliot (https://github.com/elliottower)\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "\n",
        "import supersuit as ss\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.ppo import CnnPolicy, MlpPolicy\n",
        "\n",
        "from pettingzoo.butterfly import knights_archers_zombies_v10\n",
        "\n",
        "\n",
        "def train(env_fn, steps: int = 10_000, seed: int | None = 0, **env_kwargs):\n",
        "    # Train a single model to play as each agent in an AEC environment\n",
        "    env = env_fn.parallel_env(**env_kwargs)\n",
        "\n",
        "    # Add black death wrapper so the number of agents stays constant\n",
        "    # MarkovVectorEnv does not support environments with varying numbers of active agents unless black_death is set to True\n",
        "    env = ss.black_death_v3(env)\n",
        "\n",
        "    # Pre-process using SuperSuit\n",
        "    visual_observation = not env.unwrapped.vector_state\n",
        "    if visual_observation:\n",
        "        # If the observation space is visual, reduce the color channels, resize from 512px to 84px, and apply frame stacking\n",
        "        env = ss.color_reduction_v0(env, mode=\"B\")\n",
        "        env = ss.resize_v1(env, x_size=84, y_size=84)\n",
        "        env = ss.frame_stack_v1(env, 3)\n",
        "\n",
        "    env.reset(seed=seed)\n",
        "\n",
        "    print(f\"Starting training on {str(env.metadata['name'])}.\")\n",
        "\n",
        "    env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
        "    env = ss.concat_vec_envs_v1(env, 8, num_cpus=1, base_class=\"stable_baselines3\")\n",
        "\n",
        "    # Use a CNN policy if the observation space is visual\n",
        "    model = PPO(\n",
        "        CnnPolicy if visual_observation else MlpPolicy,\n",
        "        env,\n",
        "        verbose=3,\n",
        "        batch_size=256,\n",
        "    )\n",
        "\n",
        "    model.learn(total_timesteps=steps)\n",
        "\n",
        "    model.save(f\"{env.unwrapped.metadata.get('name')}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
        "\n",
        "    print(\"Model has been saved.\")\n",
        "\n",
        "    print(f\"Finished training on {str(env.unwrapped.metadata['name'])}.\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "\n",
        "def eval(env_fn, num_games: int = 100, render_mode: str | None = None, **env_kwargs):\n",
        "    # Evaluate a trained agent vs a random agent\n",
        "    env = env_fn.env(render_mode=render_mode, **env_kwargs)\n",
        "\n",
        "    # Pre-process using SuperSuit\n",
        "    visual_observation = not env.unwrapped.vector_state\n",
        "    if visual_observation:\n",
        "        # If the observation space is visual, reduce the color channels, resize from 512px to 84px, and apply frame stacking\n",
        "        env = ss.color_reduction_v0(env, mode=\"B\")\n",
        "        env = ss.resize_v1(env, x_size=84, y_size=84)\n",
        "        env = ss.frame_stack_v1(env, 3)\n",
        "\n",
        "    print(\n",
        "        f\"\\nStarting evaluation on {str(env.metadata['name'])} (num_games={num_games}, render_mode={render_mode})\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        latest_policy = max(\n",
        "            glob.glob(f\"{env.metadata['name']}*.zip\"), key=os.path.getctime\n",
        "        )\n",
        "    except ValueError:\n",
        "        print(\"Policy not found.\")\n",
        "        exit(0)\n",
        "\n",
        "    model = PPO.load(latest_policy)\n",
        "\n",
        "    rewards = {agent: 0 for agent in env.possible_agents}\n",
        "\n",
        "    # Note: we evaluate here using an AEC environments, to allow for easy A/B testing against random policies\n",
        "    # For example, we can see here that using a random agent for archer_0 results in less points than the trained agent\n",
        "    for i in range(num_games):\n",
        "        env.reset(seed=i)\n",
        "        env.action_space(env.possible_agents[0]).seed(i)\n",
        "\n",
        "        for agent in env.agent_iter():\n",
        "            obs, reward, termination, truncation, info = env.last()\n",
        "\n",
        "            for a in env.agents:\n",
        "                rewards[a] += env.rewards[a]\n",
        "\n",
        "            if termination or truncation:\n",
        "                break\n",
        "            else:\n",
        "                if agent == env.possible_agents[0]:\n",
        "                    act = env.action_space(agent).sample()\n",
        "                else:\n",
        "                    act = model.predict(obs, deterministic=True)[0]\n",
        "            env.step(act)\n",
        "    env.close()\n",
        "\n",
        "    avg_reward = sum(rewards.values()) / len(rewards.values())\n",
        "    avg_reward_per_agent = {\n",
        "        agent: rewards[agent] / num_games for agent in env.possible_agents\n",
        "    }\n",
        "    print(f\"Avg reward: {avg_reward}\")\n",
        "    print(\"Avg reward per agent, per game: \", avg_reward_per_agent)\n",
        "    print(\"Full rewards: \", rewards)\n",
        "    return avg_reward\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env_fn = knights_archers_zombies_v10\n",
        "\n",
        "    # Set vector_state to false in order to use visual observations (significantly longer training time)\n",
        "    env_kwargs = dict(max_cycles=100, max_zombies=4, vector_state=True)\n",
        "\n",
        "    # Train a model (takes ~5 minutes on a laptop CPU)\n",
        "    train(env_fn, steps=81_920, seed=0, **env_kwargs)\n",
        "\n",
        "    # Evaluate 10 games (takes ~10 seconds on a laptop CPU)\n",
        "    eval(env_fn, num_games=10, render_mode=None, **env_kwargs)\n",
        "\n",
        "    # Watch 2 games (takes ~10 seconds on a laptop CPU)\n",
        "    eval(env_fn, num_games=2, render_mode=\"human\", **env_kwargs)"
      ],
      "metadata": {
        "id": "QTY7yBHNzsJe",
        "outputId": "0349dd5f-577b-4dd5-d443-29375cf3b898",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'supersuit'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-abc4450f4c58>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msupersuit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCnnPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMlpPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'supersuit'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import supersuit as ss\n",
        "from pettingzoo.atari import tennis_v3\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import VecMonitor\n",
        "from pettingzoo.utils.conversions import aec_to_parallel\n",
        "\n",
        "# Create the PettingZoo environment\n",
        "env = tennis_v3.env()\n",
        "\n",
        "# Apply Supersuit wrappers to make the environment compatible with Stable Baselines3\n",
        "env = ss.max_observation_v0(env, 2)  # Ensure observations are the same size\n",
        "env = ss.pad_action_space_v0(env)     # Ensure action spaces are the same size\n",
        "env = aec_to_parallel(env)\n",
        "#env = ss.pettingzoo_env_to_vec_env_v1(env)  # Convert to vectorized environment\n",
        "#env = VecMonitor(env)  # Monitor to keep track of rewards and other info\n",
        "\n",
        "# Create the model using the CNN policy for processing image observations\n",
        "model = PPO('CnnPolicy', env, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=500000)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"ppo_tennis_selfplay\")\n",
        "\n",
        "# Load the trained model\n",
        "model = PPO.load(\"ppo_tennis_selfplay\")\n",
        "\n",
        "# Evaluation loop\n",
        "env = tennis_v3.env()\n",
        "env = ss.max_observation_v0(env, 2)\n",
        "env = ss.pad_action_space_v0(env)\n",
        "env.reset()\n",
        "\n",
        "for agent in env.agent_iter():\n",
        "    observation, reward, done, info = env.last()\n",
        "    if done:\n",
        "        action = None\n",
        "    else:\n",
        "        # Use the trained model to predict actions\n",
        "        action, _ = model.predict(observation, deterministic=True)\n",
        "    env.step(action)\n",
        "    env.render()"
      ],
      "metadata": {
        "id": "LH1FW53I0yil",
        "outputId": "138a3262-2fd1-4584-c3a3-9bbfbdc679a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The environment is of type <class 'pettingzoo.utils.conversions.aec_to_parallel_wrapper'>, not a Gymnasium environment. In this case, we expect OpenAI Gym to be installed and the environment to be an OpenAI Gym environment.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-3d4f8a97efc5>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Create the model using the CNN policy for processing image observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CnnPolicy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, rollout_buffer_class, rollout_buffer_kwargs, target_kl, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0m_init_setup_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     ):\n\u001b[0;32m--> 109\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, rollout_buffer_class, rollout_buffer_kwargs, stats_window_size, tensorboard_log, monitor_wrapper, policy_kwargs, verbose, seed, device, _init_setup_model, supported_action_spaces)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0msupported_action_spaces\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpace\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     ):\n\u001b[0;32m---> 85\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_make_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor_wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36m_wrap_env\u001b[0;34m(env, verbose, monitor_wrapper)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVecEnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# Patch to support gym 0.21/0.26 and gymnasium\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_patch_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMonitor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmonitor_wrapper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py\u001b[0m in \u001b[0;36m_patch_env\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgym_installed\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;34mf\"The environment is of type {type(env)}, not a Gymnasium \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;34mf\"environment. In this case, we expect OpenAI Gym to be \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The environment is of type <class 'pettingzoo.utils.conversions.aec_to_parallel_wrapper'>, not a Gymnasium environment. In this case, we expect OpenAI Gym to be installed and the environment to be an OpenAI Gym environment."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.env import PettingZooEnv\n",
        "from pettingzoo.atari import tennis_v3\n",
        "from ray.tune.registry import register_env\n",
        "from ray.rllib.agents.ppo import PPOTrainer\n",
        "\n",
        "# Initialize Ray\n",
        "ray.init()\n",
        "\n",
        "# Environment creator function\n",
        "def env_creator(config):\n",
        "    env = tennis_v3.env()\n",
        "    return env\n",
        "\n",
        "# Register the environment with RLlib\n",
        "register_env(\"tennis_v3\", lambda config: PettingZooEnv(env_creator(config)))\n",
        "\n",
        "# Create an instance of the environment to extract spaces\n",
        "temp_env = PettingZooEnv(env_creator({}))\n",
        "obs_space = temp_env.observation_space\n",
        "act_space = temp_env.action_space\n",
        "\n",
        "# Define the policies\n",
        "policies = {\n",
        "    \"shared_policy\": (None, obs_space, act_space, {})\n",
        "}\n",
        "\n",
        "# Policy mapping function\n",
        "def policy_mapping_fn(agent_id, episode, **kwargs):\n",
        "    return \"shared_policy\"  # All agents use the same policy (self-play)\n",
        "\n",
        "# RLlib configuration\n",
        "config = {\n",
        "    \"env\": \"tennis_v3\",\n",
        "    \"env_config\": {},\n",
        "    \"framework\": \"torch\",  # Use \"tf\" if you prefer TensorFlow\n",
        "    \"num_workers\": 1,      # Increase if you have more CPUs\n",
        "    \"num_gpus\": 0,         # Set to 1 if you have a GPU\n",
        "    \"multiagent\": {\n",
        "        \"policies\": policies,\n",
        "        \"policy_mapping_fn\": policy_mapping_fn,\n",
        "    },\n",
        "    \"lr\": 1e-4,\n",
        "    \"train_batch_size\": 4000,\n",
        "    \"rollout_fragment_length\": 200,\n",
        "    \"sgd_minibatch_size\": 128,\n",
        "    \"num_sgd_iter\": 10,\n",
        "    \"clip_param\": 0.1,\n",
        "}\n",
        "\n",
        "# Start training\n",
        "results = tune.run(\n",
        "    \"PPO\",\n",
        "    config=config,\n",
        "    stop={\"timesteps_total\": 500000},\n",
        "    checkpoint_at_end=True,\n",
        ")\n",
        "\n",
        "# Get the last checkpoint\n",
        "checkpoints = results.get_trial_checkpoints_paths(\n",
        "    trial=results.get_best_trial(\"episode_reward_mean\", mode=\"max\"),\n",
        "    metric=\"episode_reward_mean\"\n",
        ")\n",
        "checkpoint_path = checkpoints[0][0]\n",
        "\n",
        "# Load the trained agent\n",
        "agent = PPOTrainer(config=config)\n",
        "agent.restore(checkpoint_path)\n",
        "\n",
        "# Evaluation loop\n",
        "env = PettingZooEnv(env_creator({}))\n",
        "env.reset()\n",
        "\n",
        "for agent_id in env.agent_iter():\n",
        "    observation, reward, done, info = env.last()\n",
        "    if done:\n",
        "        action = None\n",
        "    else:\n",
        "        action = agent.compute_single_action(observation, policy_id=\"shared_policy\")\n",
        "    env.step(action)\n",
        "    env.render()\n",
        "\n",
        "# Shutdown Ray\n",
        "ray.shutdown()\n"
      ],
      "metadata": {
        "id": "EL6yKRU6iy7q",
        "outputId": "2413267c-67d6-4dc0-b81f-fe624a37b0f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'ray'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-c482159b29d3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPettingZooEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpettingzoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matari\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtennis_v3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ray'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.env import PettingZooEnv\n",
        "from pettingzoo.atari import tennis_v3\n",
        "from ray.tune.registry import register_env\n",
        "from ray.rllib.algoirthms.ppo import PPOTrainer\n",
        "\n",
        "# Initialize Ray\n",
        "ray.init()\n",
        "\n",
        "# Environment creator function\n",
        "def env_creator(config):\n",
        "    env = tennis_v3.env()\n",
        "    return env\n",
        "\n",
        "# Register the environment with RLlib\n",
        "register_env(\"tennis_v3\", lambda config: PettingZooEnv(env_creator(config)))\n",
        "\n",
        "# Create an instance of the environment to extract spaces\n",
        "temp_env = PettingZooEnv(env_creator({}))\n",
        "obs_space = temp_env.observation_space\n",
        "act_space = temp_env.action_space\n",
        "\n",
        "# Define the policies\n",
        "policies = {\n",
        "    \"shared_policy\": (None, obs_space, act_space, {})\n",
        "}\n",
        "\n",
        "# Policy mapping function\n",
        "def policy_mapping_fn(agent_id, episode, **kwargs):\n",
        "    return \"shared_policy\"  # All agents use the same policy (self-play)\n",
        "\n",
        "# RLlib configuration\n",
        "config = {\n",
        "    \"env\": \"tennis_v3\",\n",
        "    \"env_config\": {},\n",
        "    \"framework\": \"torch\",  # Use \"tf\" if you prefer TensorFlow\n",
        "    \"num_workers\": 1,      # Increase if you have more CPUs\n",
        "    \"num_gpus\": 0,         # Set to 1 if you have a GPU\n",
        "    \"multiagent\": {\n",
        "        \"policies\": policies,\n",
        "        \"policy_mapping_fn\": policy_mapping_fn,\n",
        "    },\n",
        "    \"lr\": 1e-4,\n",
        "    \"train_batch_size\": 4000,\n",
        "    \"rollout_fragment_length\": 200,\n",
        "    \"sgd_minibatch_size\": 128,\n",
        "    \"num_sgd_iter\": 10,\n",
        "    \"clip_param\": 0.1,\n",
        "}\n",
        "\n",
        "# Start training\n",
        "results = tune.run(\n",
        "    \"PPO\",\n",
        "    config=config,\n",
        "    stop={\"timesteps_total\": 500000},\n",
        "    checkpoint_at_end=True,\n",
        ")\n",
        "\n",
        "# Get the last checkpoint\n",
        "checkpoints = results.get_trial_checkpoints_paths(\n",
        "    trial=results.get_best_trial(\"episode_reward_mean\", mode=\"max\"),\n",
        "    metric=\"episode_reward_mean\"\n",
        ")\n",
        "checkpoint_path = checkpoints[0][0]\n",
        "\n",
        "# Load the trained agent\n",
        "agent = PPOTrainer(config=config)\n",
        "agent.restore(checkpoint_path)\n",
        "\n",
        "# Evaluation loop\n",
        "env = PettingZooEnv(env_creator({}))\n",
        "env.reset()\n",
        "\n",
        "for agent_id in env.agent_iter():\n",
        "    observation, reward, done, info = env.last()\n",
        "    if done:\n",
        "        action = None\n",
        "    else:\n",
        "        action = agent.compute_single_action(observation, policy_id=\"shared_policy\")\n",
        "    env.step(action)\n",
        "    env.render()\n",
        "\n",
        "# Shutdown Ray\n",
        "ray.shutdown()\n"
      ],
      "metadata": {
        "id": "Nn4YW58rlwts",
        "outputId": "ca8b685b-8d73-40f1-dbfa-aa7c3b95eb58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'ray.rllib.agents'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c482159b29d3>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpettingzoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matari\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtennis_v3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPPOTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Initialize Ray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ray.rllib.agents'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Uses Ray's RLlib to train agents to play Pistonball.\n",
        "\n",
        "Author: Rohan (https://github.com/Rohan138)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "import ray\n",
        "import supersuit as ss\n",
        "from ray import tune\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
        "from ray.rllib.models import ModelCatalog\n",
        "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
        "from ray.tune.registry import register_env\n",
        "from torch import nn\n",
        "\n",
        "from pettingzoo.butterfly import pistonball_v6\n",
        "\n",
        "\n",
        "class CNNModelV2(TorchModelV2, nn.Module):\n",
        "    def __init__(self, obs_space, act_space, num_outputs, *args, **kwargs):\n",
        "        TorchModelV2.__init__(self, obs_space, act_space, num_outputs, *args, **kwargs)\n",
        "        nn.Module.__init__(self)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, [8, 8], stride=(4, 4)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, [4, 4], stride=(2, 2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, [3, 3], stride=(1, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            (nn.Linear(3136, 512)),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.policy_fn = nn.Linear(512, num_outputs)\n",
        "        self.value_fn = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, input_dict, state, seq_lens):\n",
        "        model_out = self.model(input_dict[\"obs\"].permute(0, 3, 1, 2))\n",
        "        self._value_out = self.value_fn(model_out)\n",
        "        return self.policy_fn(model_out), state\n",
        "\n",
        "    def value_function(self):\n",
        "        return self._value_out.flatten()\n",
        "\n",
        "\n",
        "def env_creator(args):\n",
        "    env = pistonball_v6.parallel_env(\n",
        "        n_pistons=20,\n",
        "        time_penalty=-0.1,\n",
        "        continuous=True,\n",
        "        random_drop=True,\n",
        "        random_rotate=True,\n",
        "        ball_mass=0.75,\n",
        "        ball_friction=0.3,\n",
        "        ball_elasticity=1.5,\n",
        "        max_cycles=125,\n",
        "    )\n",
        "    env = ss.color_reduction_v0(env, mode=\"B\")\n",
        "    env = ss.dtype_v0(env, \"float32\")\n",
        "    env = ss.resize_v1(env, x_size=84, y_size=84)\n",
        "    env = ss.normalize_obs_v0(env, env_min=0, env_max=1)\n",
        "    env = ss.frame_stack_v1(env, 3)\n",
        "    return env\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     ray.init()\n",
        "\n",
        "#     env_name = \"pistonball_v6\"\n",
        "\n",
        "#     register_env(env_name, lambda config: ParallelPettingZooEnv(env_creator(config)))\n",
        "#     ModelCatalog.register_custom_model(\"CNNModelV2\", CNNModelV2)\n",
        "\n",
        "#     config = (\n",
        "#         PPOConfig()\n",
        "#         .environment(env=env_name, clip_actions=True)\n",
        "#         .rollouts(num_rollout_workers=4, rollout_fragment_length=128)\n",
        "#         .training(\n",
        "#             train_batch_size=512,\n",
        "#             lr=2e-5,\n",
        "#             gamma=0.99,\n",
        "#             lambda_=0.9,\n",
        "#             use_gae=True,\n",
        "#             clip_param=0.4,\n",
        "#             grad_clip=None,\n",
        "#             entropy_coeff=0.1,\n",
        "#             vf_loss_coeff=0.25,\n",
        "#             sgd_minibatch_size=64,\n",
        "#             num_sgd_iter=10,\n",
        "#         )\n",
        "#         .debugging(log_level=\"ERROR\")\n",
        "#         .framework(framework=\"torch\")\n",
        "#         .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
        "#     )\n",
        "\n",
        "#     tune.run(\n",
        "#         \"PPO\",\n",
        "#         name=\"PPO\",\n",
        "#         stop={\"timesteps_total\": 5000000 if not os.environ.get(\"CI\") else 50000},\n",
        "#         checkpoint_freq=10,\n",
        "#         storage_path=\"~/ray_results/\" + env_name,\n",
        "#         config=config.to_dict(),\n",
        "#     )"
      ],
      "metadata": {
        "id": "fsA0mDWpmO63",
        "outputId": "c7cef102-47a4-4f4d-b4d7-973ca1936c31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ray.init()\n",
        "\n",
        "env_name = \"pistonball_v6\"\n",
        "\n",
        "register_env(env_name, lambda config: ParallelPettingZooEnv(env_creator(config)))\n",
        "ModelCatalog.register_custom_model(\"CNNModelV2\", CNNModelV2)\n",
        "\n",
        "config = (\n",
        "    PPOConfig()\n",
        "    .environment(env=env_name, clip_actions=True)\n",
        "    .rollouts(num_rollout_workers=4, rollout_fragment_length=128)\n",
        "    .training(\n",
        "        train_batch_size=512,\n",
        "        lr=2e-5,\n",
        "        gamma=0.99,\n",
        "        lambda_=0.9,\n",
        "        use_gae=True,\n",
        "        clip_param=0.4,\n",
        "        grad_clip=None,\n",
        "        entropy_coeff=0.1,\n",
        "        vf_loss_coeff=0.25,\n",
        "        sgd_minibatch_size=64,\n",
        "        num_sgd_iter=10,\n",
        "    )\n",
        "    .debugging(log_level=\"ERROR\")\n",
        "    .framework(framework=\"torch\")\n",
        "    .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
        ")\n",
        "\n",
        "tune.run(\n",
        "    \"PPO\",\n",
        "    name=\"PPO\",\n",
        "    stop={\"timesteps_total\": 5000000 if not os.environ.get(\"CI\") else 50000},\n",
        "    checkpoint_freq=10,\n",
        "    storage_path=\"~/ray_results/\" + env_name,\n",
        "    config=config.to_dict(),\n",
        ")"
      ],
      "metadata": {
        "id": "ZH-ei03GnEtM",
        "outputId": "b7d90c4e-a272-4ace-c2dd-2af7334d8d96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = _posixsubprocess.fork_exec(\n",
            "/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = _posixsubprocess.fork_exec(\n",
            "2024-09-14 04:11:23,959\tINFO worker.py:1783 -- Started a local Ray instance.\n",
            "2024-09-14 04:11:26,018\tWARNING deprecation.py:50 -- DeprecationWarning: `rollouts` has been deprecated. Use `AlgorithmConfig.env_runners(..)` instead. This will raise an error in the future!\n",
            "2024-09-14 04:11:26,020\tWARNING deprecation.py:50 -- DeprecationWarning: `AlgorithmConfig.env_runners(num_rollout_workers)` has been deprecated. Use `AlgorithmConfig.env_runners(num_env_runners)` instead. This will raise an error in the future!\n",
            "/usr/local/lib/python3.10/dist-packages/ray/tune/tune.py:675: DeprecationWarning: checkpoint_freq is deprecated and will be removed. use checkpoint_config.checkpoint_frequency instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:164: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:188: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------+\n",
            "| Configuration for experiment     PPO                   |\n",
            "+--------------------------------------------------------+\n",
            "| Search algorithm                 BasicVariantGenerator |\n",
            "| Scheduler                        FIFOScheduler         |\n",
            "| Number of trials                 1                     |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "View detailed results here: /root/ray_results/pistonball_v6/PPO\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-09-14_04-11-18_580819_4740/artifacts/2024-09-14_04-11-26/PPO/driver_artifacts`\n",
            "\n",
            "Trial status: 1 PENDING\n",
            "Current time: 2024-09-14 04:11:26. Total running time: 0s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------+\n",
            "| Trial name                      status   |\n",
            "+------------------------------------------+\n",
            "| PPO_pistonball_v6_6907c_00000   PENDING  |\n",
            "+------------------------------------------+\n",
            "Trial status: 1 PENDING\n",
            "Current time: 2024-09-14 04:11:56. Total running time: 30s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------+\n",
            "| Trial name                      status   |\n",
            "+------------------------------------------+\n",
            "| PPO_pistonball_v6_6907c_00000   PENDING  |\n",
            "+------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-09-14 04:12:26,639\tWARNING insufficient_resources_manager.py:163 -- Ignore this message if the cluster is autoscaling. No trial is running and no new trial has been started within the last 60 seconds. This could be due to the cluster not having enough resources available. You asked for 5.0 CPUs and 0 GPUs per trial, but the cluster only has 2.0 CPUs and 1.0 GPUs available. Stop the tuning and adjust the required resources (e.g. via the `ScalingConfig` or `resources_per_trial`, or `num_workers` for rllib), or add more resources to your cluster.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial status: 1 PENDING\n",
            "Current time: 2024-09-14 04:12:26. Total running time: 1min 0s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------+\n",
            "| Trial name                      status   |\n",
            "+------------------------------------------+\n",
            "| PPO_pistonball_v6_6907c_00000   PENDING  |\n",
            "+------------------------------------------+\n",
            "Trial status: 1 PENDING\n",
            "Current time: 2024-09-14 04:12:56. Total running time: 1min 30s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------+\n",
            "| Trial name                      status   |\n",
            "+------------------------------------------+\n",
            "| PPO_pistonball_v6_6907c_00000   PENDING  |\n",
            "+------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-09-14 04:13:26,656\tWARNING insufficient_resources_manager.py:163 -- Ignore this message if the cluster is autoscaling. No trial is running and no new trial has been started within the last 60 seconds. This could be due to the cluster not having enough resources available. You asked for 5.0 CPUs and 0 GPUs per trial, but the cluster only has 2.0 CPUs and 1.0 GPUs available. Stop the tuning and adjust the required resources (e.g. via the `ScalingConfig` or `resources_per_trial`, or `num_workers` for rllib), or add more resources to your cluster.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial status: 1 PENDING\n",
            "Current time: 2024-09-14 04:13:26. Total running time: 2min 0s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------+\n",
            "| Trial name                      status   |\n",
            "+------------------------------------------+\n",
            "| PPO_pistonball_v6_6907c_00000   PENDING  |\n",
            "+------------------------------------------+\n",
            "Trial status: 1 PENDING\n",
            "Current time: 2024-09-14 04:13:56. Total running time: 2min 30s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------+\n",
            "| Trial name                      status   |\n",
            "+------------------------------------------+\n",
            "| PPO_pistonball_v6_6907c_00000   PENDING  |\n",
            "+------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-09-14 04:14:26,657\tWARNING insufficient_resources_manager.py:163 -- Ignore this message if the cluster is autoscaling. No trial is running and no new trial has been started within the last 60 seconds. This could be due to the cluster not having enough resources available. You asked for 5.0 CPUs and 0 GPUs per trial, but the cluster only has 2.0 CPUs and 1.0 GPUs available. Stop the tuning and adjust the required resources (e.g. via the `ScalingConfig` or `resources_per_trial`, or `num_workers` for rllib), or add more resources to your cluster.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial status: 1 PENDING\n",
            "Current time: 2024-09-14 04:14:26. Total running time: 3min 0s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------+\n",
            "| Trial name                      status   |\n",
            "+------------------------------------------+\n",
            "| PPO_pistonball_v6_6907c_00000   PENDING  |\n",
            "+------------------------------------------+\n",
            "Trial status: 1 PENDING\n",
            "Current time: 2024-09-14 04:14:56. Total running time: 3min 30s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------+\n",
            "| Trial name                      status   |\n",
            "+------------------------------------------+\n",
            "| PPO_pistonball_v6_6907c_00000   PENDING  |\n",
            "+------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-09-14 04:15:26,664\tWARNING insufficient_resources_manager.py:163 -- Ignore this message if the cluster is autoscaling. No trial is running and no new trial has been started within the last 60 seconds. This could be due to the cluster not having enough resources available. You asked for 5.0 CPUs and 0 GPUs per trial, but the cluster only has 2.0 CPUs and 1.0 GPUs available. Stop the tuning and adjust the required resources (e.g. via the `ScalingConfig` or `resources_per_trial`, or `num_workers` for rllib), or add more resources to your cluster.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial status: 1 PENDING\n",
            "Current time: 2024-09-14 04:15:26. Total running time: 4min 0s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------+\n",
            "| Trial name                      status   |\n",
            "+------------------------------------------+\n",
            "| PPO_pistonball_v6_6907c_00000   PENDING  |\n",
            "+------------------------------------------+\n",
            "Trial status: 1 PENDING\n",
            "Current time: 2024-09-14 04:15:56. Total running time: 4min 30s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------+\n",
            "| Trial name                      status   |\n",
            "+------------------------------------------+\n",
            "| PPO_pistonball_v6_6907c_00000   PENDING  |\n",
            "+------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-09-14 04:16:26,670\tWARNING insufficient_resources_manager.py:163 -- Ignore this message if the cluster is autoscaling. No trial is running and no new trial has been started within the last 60 seconds. This could be due to the cluster not having enough resources available. You asked for 5.0 CPUs and 0 GPUs per trial, but the cluster only has 2.0 CPUs and 1.0 GPUs available. Stop the tuning and adjust the required resources (e.g. via the `ScalingConfig` or `resources_per_trial`, or `num_workers` for rllib), or add more resources to your cluster.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial status: 1 PENDING\n",
            "Current time: 2024-09-14 04:16:26. Total running time: 5min 0s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------+\n",
            "| Trial name                      status   |\n",
            "+------------------------------------------+\n",
            "| PPO_pistonball_v6_6907c_00000   PENDING  |\n",
            "+------------------------------------------+\n",
            "Trial status: 1 PENDING\n",
            "Current time: 2024-09-14 04:16:56. Total running time: 5min 30s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------+\n",
            "| Trial name                      status   |\n",
            "+------------------------------------------+\n",
            "| PPO_pistonball_v6_6907c_00000   PENDING  |\n",
            "+------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CFo4Ls-WuT57"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}