{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNWyUZ7zAqpYP55tVgcRUCe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuds/rl-atari-tennis/blob/main/%5BAtari%20Tennis%5D%20Single-Agent%20Reinforcement%20Learning%20DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Atari Tennis] Single-Agent Reinforcement Learning DQN"
      ],
      "metadata": {
        "id": "ob-2hGT3jNdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig"
      ],
      "metadata": {
        "id": "Fy1WJ1R-iP4V",
        "outputId": "f508a1d9-8cc5-4d02-ed09-7d28ac053e42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium gymnasium[atari] autorom"
      ],
      "metadata": {
        "id": "7U_qTvGlj2oy",
        "outputId": "529a1534-760f-48ef-c4f0-765513058d29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: autorom in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: shimmy<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (0.2.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom) (2.32.3)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (0.8.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (2024.8.30)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (6.4.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!AutoROM --accept-license"
      ],
      "metadata": {
        "id": "Le7dztPUkVz4",
        "outputId": "904ed611-b7e3-4223-fd3b-7fb68af57f32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoROM will download the Atari 2600 ROMs.\n",
            "They will be installed to:\n",
            "\t/usr/local/lib/python3.10/dist-packages/AutoROM/roms\n",
            "\n",
            "Existing ROMs will be overwritten.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3"
      ],
      "metadata": {
        "id": "NmOqw4UAmAoV",
        "outputId": "edcbbb12-818d-427b-9964-6cb2de78fd47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.10/dist-packages (2.3.2)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.4.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2024.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import platform\n",
        "import torch\n",
        "import numpy\n",
        "import stable_baselines3\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "from stable_baselines3.common.vec_env import VecFrameStack\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.callbacks import CallbackList\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder\n",
        "from stable_baselines3.common.vec_env import VecTransposeImage\n",
        "import matplotlib.pyplot\n",
        "import matplotlib\n",
        "import os\n",
        "import gymnasium\n",
        "from importlib.metadata import version\n",
        "from datetime import datetime\n",
        "import csv\n",
        "import time\n",
        "import google.colab.drive"
      ],
      "metadata": {
        "id": "o89JlEiexh1-",
        "outputId": "3a5e69fa-0b1a-40d9-8daa-39ce06834287",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n",
            "  from jax import xla_computation as _xla_computation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Python Version: {platform.python_version()}\")\n",
        "print(f\"Torch Version: {version('torch')}\")\n",
        "print(f\"Is Cuda Available: {torch.cuda.is_available()}\")\n",
        "print(f\"Cuda Version: {torch.version.cuda}\")\n",
        "print(f\"Gymnasium Version: {version('gymnasium')}\")\n",
        "print(f\"Numpy Version: {version('numpy')}\")\n",
        "print(f\"Stable Baselines3 Version: {version('stable_baselines3')}\")"
      ],
      "metadata": {
        "id": "KRrJSPuDiDN4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d784df4f-cebb-476a-a1ec-d6229ac1bee7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python Version: 3.10.12\n",
            "Torch Version: 2.4.1+cu121\n",
            "Is Cuda Available: True\n",
            "Cuda Version: 12.1\n",
            "Gymnasium Version: 0.29.1\n",
            "Numpy Version: 1.26.4\n",
            "Stable Baselines3 Version: 2.3.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gdrive_path = \"/content/gdrive\"\n",
        "google.colab.drive.mount(gdrive_path, force_remount=True)"
      ],
      "metadata": {
        "id": "LKxm5e7iGW_-",
        "outputId": "430733b4-6f48-49e4-9a2b-39d089baab09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rl_type = \"DQN\"\n",
        "env_str = \"ALE/Tennis-v5\"\n",
        "trained_model = \"final_atari_tennis_dqn\"\n",
        "log_dir = \"{}/MyDrive/Finding Theta/logs/{}/{}\".format(gdrive_path,\n",
        "                                                       env_str,\n",
        "                                                       rl_type)\n",
        "training_data_path = os.path.join(log_dir, \"training jobs\")\n",
        "time_folder = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "model_folder_path = os.path.join(log_dir, \"training jobs\", time_folder)"
      ],
      "metadata": {
        "id": "aR7w3BqcFiIO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Folders\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "os.makedirs(training_data_path, exist_ok=True)\n",
        "os.makedirs(model_folder_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "TKSfoDHMdbsf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = make_atari_env(env_str, n_envs=1, seed=0)\n",
        "print(\"Observation Space Size: \", env.observation_space.shape)\n",
        "print('Actions Space: ', env.action_space)\n",
        "env.close()"
      ],
      "metadata": {
        "id": "T78JvLBVdmQO",
        "outputId": "08434607-f923-45cf-b821-6628ef2f5510",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation Space Size:  (84, 84, 1)\n",
            "Actions Space:  Discrete(18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gymnasium.make(env_str)\n",
        "print(\"Observation Space Size: \", env.observation_space.shape)\n",
        "print('Actions Space: ', env.action_space)\n",
        "env.close()"
      ],
      "metadata": {
        "id": "5vCHAlsgg3lu",
        "outputId": "17093bea-040e-415c-c419-aef77ec55c88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation Space Size:  (210, 160, 3)\n",
            "Actions Space:  Discrete(18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Environment Wrapper Arguments\n",
        "# Disable Frame Skipping\n",
        "# https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\n",
        "# https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.html\n",
        "env_val_wrap_args = {\"frame_skip\": 0, \"noop_max\": 30}"
      ],
      "metadata": {
        "id": "1bgRLf7cUY-8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoRecordCallback(BaseCallback):\n",
        "    def __init__(\n",
        "        self,\n",
        "        save_path: str,\n",
        "        video_length: int,\n",
        "        save_freq: int = 5_000,\n",
        "        name_prefix: str =\"rl_model\",\n",
        "        verbose: int = 0):\n",
        "\n",
        "        super().__init__(verbose)\n",
        "        self.save_freq = save_freq\n",
        "        self.video_length = video_length\n",
        "        self.save_path = save_path\n",
        "        self.name_prefix = name_prefix\n",
        "        # Those variables will be accessible in the callback\n",
        "        # (they are defined in the base class)\n",
        "        # The RL model\n",
        "        # self.model = None  # type: BaseAlgorithm\n",
        "        # An alias for self.model.get_env(), the environment used for training\n",
        "        # self.training_env # type: VecEnv\n",
        "        # Number of time the callback was called\n",
        "        # self.n_calls = 0  # type: int\n",
        "        # num_timesteps = n_envs * n times env.step() was called\n",
        "        # self.num_timesteps = 0  # type: int\n",
        "        # local and global variables\n",
        "        # self.locals = {}  # type: Dict[str, Any]\n",
        "        # self.globals = {}  # type: Dict[str, Any]\n",
        "        # The logger object, used to report things in the terminal\n",
        "        # self.logger # type: stable_baselines3.common.logger.Logger\n",
        "        # Sometimes, for event callback, it is useful\n",
        "        # to have access to the parent object\n",
        "        # self.parent = None  # type: Optional[BaseCallback]\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.save_freq == 0:\n",
        "\n",
        "          name_prefix = f\"{self.name_prefix}_{self.num_timesteps}\"\n",
        "\n",
        "          # Record video of the best model playing Atari's Tennis\n",
        "          rec_val = make_atari_env(\n",
        "              env_str,\n",
        "              n_envs=1,\n",
        "              seed=1,\n",
        "              wrapper_kwargs=env_val_wrap_args)\n",
        "          rec_val = VecTransposeImage(rec_val)\n",
        "          rec_val = VecFrameStack(rec_val, n_stack=4)\n",
        "          rec_val = VecVideoRecorder(rec_val,\n",
        "                                    self.save_path,\n",
        "                                    video_length=self.video_length,\n",
        "                                    record_video_trigger=lambda x: x == 0,\n",
        "                                    name_prefix=name_prefix)\n",
        "\n",
        "          obs = rec_val.reset()\n",
        "          for _ in range(self.video_length):\n",
        "              action, _states = self.model.predict(obs)\n",
        "              obs, rewards, dones, info = rec_val.step(action)\n",
        "              rec_val.render()\n",
        "              if dones:\n",
        "                break\n",
        "\n",
        "          rec_val.close()\n",
        "        return True"
      ],
      "metadata": {
        "id": "XxdvhhHvw-1O"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Environments\n",
        "# Create the Training Atari Tennis environment with appropriate wrappers\n",
        "env = make_atari_env(env_str, n_envs=4)\n",
        "env = VecTransposeImage(env)\n",
        "env = VecFrameStack(env, n_stack=4)\n",
        "\n",
        "# Create the Evaluation Atari Tennis environment with appropriate wrappers\n",
        "# Disable Frame Skip\n",
        "env_val = make_atari_env(\n",
        "    env_str,\n",
        "    n_envs=1,\n",
        "    seed=1,\n",
        "    wrapper_kwargs=env_val_wrap_args)\n",
        "env_val = VecTransposeImage(env_val)\n",
        "env_val = VecFrameStack(env_val, n_stack=4)"
      ],
      "metadata": {
        "id": "OxXLoo7erYVa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Callbacks\n",
        "# Create Evaluation Callback\n",
        "# eval_freq - can cause learning instability if set to low\n",
        "eval_callback = EvalCallback(\n",
        "    env_val,\n",
        "    best_model_save_path=model_folder_path,\n",
        "    log_path=model_folder_path,\n",
        "    eval_freq=5_000,\n",
        "    render=False,\n",
        "    deterministic=True,\n",
        "    n_eval_episodes=5)\n",
        "\n",
        "# Create Checkpoint Callback\n",
        "checkpoint_callback = CheckpointCallback(\n",
        "    save_freq=5_000,\n",
        "    save_path=os.path.join(model_folder_path, \"checkpoints\"),\n",
        "    name_prefix=\"atari_tennis_dqn\",\n",
        "    save_replay_buffer=False,\n",
        "    save_vecnormalize=False,\n",
        ")\n",
        "\n",
        "video_record_callback = VideoRecordCallback(\n",
        "    save_path=os.path.join(model_folder_path, \"videos\"),\n",
        "    video_length=2_500,\n",
        "    save_freq=5_000,\n",
        "    name_prefix=\"atari_tennis_dqn\")\n",
        "\n",
        "# Create the callback list\n",
        "callbackList = CallbackList([checkpoint_callback, video_record_callback, eval_callback])"
      ],
      "metadata": {
        "id": "_wLI5Z46rstT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvPXWTCHiO69"
      },
      "outputs": [],
      "source": [
        "# Initialize DQN\n",
        "model = DQN('CnnPolicy', env,\n",
        "            verbose=0,\n",
        "            batch_size=128,\n",
        "            buffer_size=750_000,\n",
        "            tensorboard_log=os.path.join(log_dir, \"tensorboard\"))\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=10_000_000,\n",
        "            progress_bar=False,\n",
        "            callback=eval_callback)\n",
        "\n",
        "# Save the trained model\n",
        "model.save(os.path.join(model_folder_path, trained_model))\n",
        "\n",
        "env.close()\n",
        "env_val.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Get model path from last training job (uncomment if training job interrupted)\n",
        "# # List all entries in the directory\n",
        "# entries = os.listdir(training_data_path)\n",
        "\n",
        "# # Filter out only directories\n",
        "# folders = [entry for entry in entries if os.path.isdir(os.path.join(training_data_path, entry))]\n",
        "\n",
        "# # Sort the folders alphabetically\n",
        "# folders.sort()\n",
        "\n",
        "# # Get the last folder\n",
        "# model_folder_path = os.path.join(training_data_path, folders[-1])\n",
        "# print(model_folder_path)"
      ],
      "metadata": {
        "id": "DFZCWZfHlSv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Evaluation Atari Tennis environment with appropriate wrappers\n",
        "# Disable Frame Skip\n",
        "env_val = make_atari_env(env_str,\n",
        "                         n_envs=1,\n",
        "                         seed=1,\n",
        "                         wrapper_kwargs=env_val_wrap_args)\n",
        "env_val = VecTransposeImage(env_val)\n",
        "env_val = VecFrameStack(env_val, n_stack=4)\n",
        "\n",
        "# Load the best model\n",
        "best_model_path = os.path.join(model_folder_path, \"best_model\")\n",
        "best_model = DQN.load(best_model_path, env=env_val)\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(\n",
        "    best_model,\n",
        "    env_val,\n",
        "    n_eval_episodes=5)\n",
        "print(f\"Best Model - Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "best_metrics_path = os.path.join(log_dir, \"best_model_metrics.csv\")\n",
        "\n",
        "# Create Best Model Metrics file if not there\n",
        "if(not os.path.isfile(best_metrics_path)):\n",
        "  with open(best_metrics_path, 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"run_date\",\n",
        "                     \"batch_size\",\n",
        "                     \"ent_coef\",\n",
        "                     \"learning_rate\",\n",
        "                     \"num_timesteps\",\n",
        "                     \"mean_reward\",\n",
        "                     \"std_reward\",\n",
        "                     \"n_steps\",\n",
        "                     \"n_envs\",\n",
        "                     \"gamma\",\n",
        "                     \"gae_lambda\",\n",
        "                     \"clip_range_vf\",\n",
        "                     \"n_epochs\"])\n",
        "\n",
        "new_data = [os.path.basename(os.path.normpath(model_folder_path)),\n",
        "            best_model.batch_size,\n",
        "            best_model.ent_coef,\n",
        "            best_model.learning_rate,\n",
        "            best_model.num_timesteps,\n",
        "            mean_reward,\n",
        "            std_reward,\n",
        "            best_model.n_steps,\n",
        "            best_model.n_envs,\n",
        "            best_model.gamma,\n",
        "            best_model.gae_lambda,\n",
        "            best_model.clip_range_vf,\n",
        "            best_model.n_epochs]\n",
        "\n",
        "with open(best_metrics_path, 'a', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(new_data)\n",
        "\n",
        "# Record video of the best model playing Atari's Tennis\n",
        "rec_val = VecVideoRecorder(env_val,\n",
        "                           os.path.join(model_folder_path, \"videos\"),\n",
        "                           video_length=5000,\n",
        "                           record_video_trigger=lambda x: x == 0,\n",
        "                           name_prefix=\"best_model_atari_tennis_dqn\")\n",
        "\n",
        "obs = rec_val.reset()\n",
        "for _ in range(5000):\n",
        "    action, _states = best_model.predict(obs)\n",
        "    obs, rewards, dones, info = rec_val.step(action)\n",
        "    rec_val.render()\n",
        "    if dones:\n",
        "      break\n",
        "\n",
        "env_val.close()\n",
        "rec_val.close()"
      ],
      "metadata": {
        "id": "OqwJT0bhcOu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Model\n",
        "print(best_model.policy)"
      ],
      "metadata": {
        "id": "5omjtaubDH1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the evaluations.npz file\n",
        "data = numpy.load(os.path.join(model_folder_path, \"evaluations.npz\"))\n",
        "\n",
        "# Extract the relevant data\n",
        "timesteps = data['timesteps']\n",
        "results = data['results']\n",
        "\n",
        "# Calculate the mean and standard deviation of the results\n",
        "mean_results = numpy.mean(results, axis=1)\n",
        "std_results = numpy.std(results, axis=1)\n",
        "\n",
        "# Plot the results\n",
        "matplotlib.pyplot.figure()\n",
        "matplotlib.pyplot.plot(timesteps, mean_results)\n",
        "matplotlib.pyplot.fill_between(timesteps,\n",
        "                               mean_results - std_results,\n",
        "                               mean_results + std_results,\n",
        "                               alpha=0.3)\n",
        "\n",
        "matplotlib.pyplot.xlabel('Timesteps')\n",
        "matplotlib.pyplot.ylabel('Mean Reward')\n",
        "matplotlib.pyplot.title(f\"{rl_type} Performance on {env_str}\")\n",
        "matplotlib.pyplot.show()"
      ],
      "metadata": {
        "id": "Xisgp9SfcONO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "If3saIYUlM7h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}