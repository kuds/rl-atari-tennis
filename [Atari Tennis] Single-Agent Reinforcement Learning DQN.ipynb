{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOCX6Mpzkzm17GcglpeE/KZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuds/rl-atari-tennis/blob/main/%5BAtari%20Tennis%5D%20Single-Agent%20Reinforcement%20Learning%20DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Atari Tennis] Single-Agent Reinforcement Learning DQN"
      ],
      "metadata": {
        "id": "ob-2hGT3jNdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig"
      ],
      "metadata": {
        "id": "Fy1WJ1R-iP4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3"
      ],
      "metadata": {
        "id": "NmOqw4UAmAoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[atari] autorom"
      ],
      "metadata": {
        "id": "7U_qTvGlj2oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!AutoROM --accept-license"
      ],
      "metadata": {
        "id": "Le7dztPUkVz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import time\n",
        "import json\n",
        "import platform\n",
        "import torch\n",
        "import numpy\n",
        "import stable_baselines3\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "from stable_baselines3.common.vec_env import VecFrameStack\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.callbacks import CallbackList\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder\n",
        "from stable_baselines3.common.vec_env import VecTransposeImage\n",
        "import matplotlib.pyplot\n",
        "import matplotlib\n",
        "import gymnasium\n",
        "from importlib.metadata import version\n",
        "from datetime import datetime\n",
        "import google.colab.drive"
      ],
      "metadata": {
        "id": "o89JlEiexh1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Python Version: {platform.python_version()}\")\n",
        "print(f\"Torch Version: {version('torch')}\")\n",
        "print(f\"Is Cuda Available: {torch.cuda.is_available()}\")\n",
        "print(f\"Cuda Version: {torch.version.cuda}\")\n",
        "print(f\"Gymnasium Version: {version('gymnasium')}\")\n",
        "print(f\"Numpy Version: {version('numpy')}\")\n",
        "print(f\"Stable Baselines3 Version: {version('stable_baselines3')}\")\n",
        "print(f\"ALE Version: {version('ale_py')}\")"
      ],
      "metadata": {
        "id": "KRrJSPuDiDN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdrive_path = \"/content/gdrive\"\n",
        "google.colab.drive.mount(gdrive_path, force_remount=True)"
      ],
      "metadata": {
        "id": "LKxm5e7iGW_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rl_type = \"DQN\"\n",
        "env_str =  \"TennisNoFrameskip-v4\" #\"ALE/Tennis-v5\"\n",
        "name_prefix = \"atari_tennis_dqn\"\n",
        "log_dir = \"{}/MyDrive/Finding Theta/logs/{}/{}\".format(gdrive_path,\n",
        "                                                       env_str,\n",
        "                                                       rl_type)\n",
        "training_data_path = os.path.join(log_dir, \"training jobs\")\n",
        "time_folder = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "model_folder_path = os.path.join(log_dir, \"training jobs\", time_folder)"
      ],
      "metadata": {
        "id": "aR7w3BqcFiIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Folders\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "os.makedirs(training_data_path, exist_ok=True)\n",
        "os.makedirs(model_folder_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "TKSfoDHMdbsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = make_atari_env(env_str, n_envs=1, seed=0)\n",
        "print(\"Observation Space Size: \", env.observation_space.shape)\n",
        "print('Actions Space: ', env.action_space)\n",
        "env.close()"
      ],
      "metadata": {
        "id": "T78JvLBVdmQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gymnasium.make(env_str)\n",
        "print(\"Observation Space Size: \", env.observation_space.shape)\n",
        "print('Actions Space: ', env.action_space)\n",
        "env.close()"
      ],
      "metadata": {
        "id": "5vCHAlsgg3lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Hyperparameters from RL Zoo\n",
        "hyperparams = {\n",
        "    \"env_str\": env_str,\n",
        "    \"rl_type\": rl_type,\n",
        "    \"eval_freq\": 2_500_000,\n",
        "    \"buffer_size\": 100_000,\n",
        "    \"total_timesteps\": 10_000_000,\n",
        "    \"train_freq\": 4,\n",
        "    \"target_update_interval\": 1_000,\n",
        "    \"gradient_steps\": 1,\n",
        "    \"exploration_fraction\": 0.1,\n",
        "    \"exploration_final_eps\": 0.01,\n",
        "    \"n_envs\": 1,\n",
        "    \"batch_size\": 32,\n",
        "    \"learning_rate\":0.0001,\n",
        "    \"learning_starts\":100_000,\n",
        "    \"optimize_memory_usage\": False,\n",
        "}\n",
        "\n",
        "# atari:\n",
        "#   env_wrapper:\n",
        "#     - stable_baselines3.common.atari_wrappers.AtariWrapper\n",
        "#   frame_stack: 4\n",
        "#   policy: 'CnnPolicy'\n",
        "#   n_timesteps: !!float 1e7\n",
        "#   buffer_size: 100000\n",
        "#   learning_rate: !!float 1e-4\n",
        "#   batch_size: 32\n",
        "#   learning_starts: 100000\n",
        "#   target_update_interval: 1000\n",
        "#   train_freq: 4\n",
        "#   gradient_steps: 1\n",
        "#   exploration_fraction: 0.1\n",
        "#   exploration_final_eps: 0.01\n",
        "#   # If True, you need to deactivate handle_timeout_termination\n",
        "#   # in the replay_buffer_kwargs\n",
        "#   optimize_memory_usage: False"
      ],
      "metadata": {
        "id": "JVb7nyiHjh0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Environment Wrapper Arguments\n",
        "# Disable Frame Skipping\n",
        "# Disable Noop Max\n",
        "# https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\n",
        "# https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.html\n",
        "\n",
        "# Training Aruguments\n",
        "env_wrap_args = {\"frame_skip\": 4}\n",
        "env_kwargs = {\"render_mode\": \"rgb_array\"}\n",
        "\n",
        "# Evaluation Aruguments\n",
        "env_val_wrap_args = {\"frame_skip\": 4, \"noop_max\": 30}\n",
        "env_val_kwargs = {\"render_mode\": \"rgb_array\"}\n",
        "\n",
        "if(env_str == \"ALE/Pong-v5\"):\n",
        "  env_wrap_args[\"frame_skip\"] = 0\n",
        "  env_val_kwargs[\"frameskip\"] = 32\n",
        "  env_val_kwargs[\"repeat_action_probability\"] = 0.0"
      ],
      "metadata": {
        "id": "1bgRLf7cUY-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(model_folder_path, 'parameters.json'), 'w') as fp:\n",
        "    parameters = {\"hyperparams\": hyperparams,\n",
        "                  \"env_wrap_args\": env_wrap_args,\n",
        "                  \"env_kwargs\": env_kwargs,\n",
        "                  \"env_val_wrap_args\": env_val_wrap_args,\n",
        "                  \"env_val_kwargs\": env_val_kwargs}\n",
        "    json.dump(parameters, fp, indent=4)"
      ],
      "metadata": {
        "id": "cSvRf1Majpl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoRecordCallback(BaseCallback):\n",
        "    def __init__(\n",
        "        self,\n",
        "        save_path: str,\n",
        "        video_length: int,\n",
        "        save_freq: int = 5_000,\n",
        "        name_prefix: str =\"rl_model\",\n",
        "        verbose: int = 0):\n",
        "\n",
        "        super().__init__(verbose)\n",
        "        self.save_freq = save_freq\n",
        "        self.video_length = video_length\n",
        "        self.save_path = save_path\n",
        "        self.name_prefix = name_prefix\n",
        "        # Those variables will be accessible in the callback\n",
        "        # (they are defined in the base class)\n",
        "        # The RL model\n",
        "        # self.model = None  # type: BaseAlgorithm\n",
        "        # An alias for self.model.get_env(), the environment used for training\n",
        "        # self.training_env # type: VecEnv\n",
        "        # Number of time the callback was called\n",
        "        # self.n_calls = 0  # type: int\n",
        "        # num_timesteps = n_envs * n times env.step() was called\n",
        "        # self.num_timesteps = 0  # type: int\n",
        "        # local and global variables\n",
        "        # self.locals = {}  # type: Dict[str, Any]\n",
        "        # self.globals = {}  # type: Dict[str, Any]\n",
        "        # The logger object, used to report things in the terminal\n",
        "        # self.logger # type: stable_baselines3.common.logger.Logger\n",
        "        # Sometimes, for event callback, it is useful\n",
        "        # to have access to the parent object\n",
        "        # self.parent = None  # type: Optional[BaseCallback]\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.save_freq == 0:\n",
        "\n",
        "          name_prefix = f\"{self.name_prefix}_{self.num_timesteps}\"\n",
        "\n",
        "          # Record video of the best model playing Atari's Environment\n",
        "          rec_val = make_atari_env(\n",
        "              env_str,\n",
        "              n_envs=1,\n",
        "              seed=1)\n",
        "          rec_val = VecFrameStack(rec_val, n_stack=4)\n",
        "          rec_val = VecTransposeImage(rec_val)\n",
        "          rec_val = VecVideoRecorder(rec_val,\n",
        "                                    self.save_path,\n",
        "                                    video_length=self.video_length,\n",
        "                                    record_video_trigger=lambda x: x == 0,\n",
        "                                    name_prefix=name_prefix)\n",
        "\n",
        "          obs = rec_val.reset()\n",
        "          for _ in range(self.video_length):\n",
        "              action, _states = self.model.predict(obs)\n",
        "              obs, rewards, dones, info = rec_val.step(action)\n",
        "              rec_val.render()\n",
        "              if dones:\n",
        "                break\n",
        "\n",
        "          rec_val.close()\n",
        "        return True"
      ],
      "metadata": {
        "id": "XxdvhhHvw-1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Training Atari Tennis environment with appropriate wrappers\n",
        "env = make_atari_env(env_str,\n",
        "                     n_envs=hyperparams[\"n_envs\"],\n",
        "                     seed=0,\n",
        "                     monitor_dir=os.path.join(model_folder_path, \"monitor\"))\n",
        "env = VecFrameStack(env, n_stack=4)\n",
        "env = VecTransposeImage(env)\n",
        "\n",
        "# Create the Evaluation Atari Tennis environment with appropriate wrappers\n",
        "env_val = make_atari_env(env_str,\n",
        "                         n_envs=1,\n",
        "                         seed=1)\n",
        "env_val = VecFrameStack(env_val, n_stack=4)\n",
        "env_val = VecTransposeImage(env_val)"
      ],
      "metadata": {
        "id": "OxXLoo7erYVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Callbacks\n",
        "# Create Evaluation Callback\n",
        "eval_callback = EvalCallback(\n",
        "    env_val,\n",
        "    best_model_save_path=model_folder_path,\n",
        "    log_path=model_folder_path,\n",
        "    eval_freq=hyperparams[\"eval_freq\"],\n",
        "    render=False,\n",
        "    deterministic=True,\n",
        "    n_eval_episodes=5)\n",
        "\n",
        "# Create Checkpoint Callback\n",
        "checkpoint_callback = CheckpointCallback(\n",
        "    save_freq=hyperparams[\"eval_freq\"],\n",
        "    save_path=os.path.join(model_folder_path, \"checkpoints\"),\n",
        "    name_prefix=name_prefix,\n",
        "    save_replay_buffer=False,\n",
        "    save_vecnormalize=False,\n",
        ")\n",
        "\n",
        "video_record_callback = VideoRecordCallback(\n",
        "    save_path=os.path.join(model_folder_path, \"videos\"),\n",
        "    video_length=2_500,\n",
        "    save_freq=hyperparams[\"eval_freq\"],\n",
        "    name_prefix=name_prefix)\n",
        "\n",
        "# Create the callback list\n",
        "callbackList = CallbackList([checkpoint_callback,\n",
        "                             video_record_callback,\n",
        "                             eval_callback])"
      ],
      "metadata": {
        "id": "_wLI5Z46rstT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvPXWTCHiO69"
      },
      "outputs": [],
      "source": [
        "# Initialize DQN\n",
        "model = DQN('CnnPolicy',\n",
        "            env,\n",
        "            verbose=1,\n",
        "            buffer_size=hyperparams[\"buffer_size\"],\n",
        "            batch_size=hyperparams[\"batch_size\"],\n",
        "            learning_rate=hyperparams[\"learning_rate\"],\n",
        "            learning_starts=hyperparams[\"learning_starts\"],\n",
        "            train_freq=hyperparams[\"train_freq\"],\n",
        "            gradient_steps=hyperparams[\"gradient_steps\"],\n",
        "            target_update_interval=hyperparams[\"target_update_interval\"],\n",
        "            exploration_fraction=hyperparams[\"exploration_fraction\"],\n",
        "            exploration_final_eps=hyperparams[\"exploration_final_eps\"],\n",
        "            optimize_memory_usage=hyperparams[\"optimize_memory_usage\"],\n",
        "            tensorboard_log=os.path.join(log_dir, \"tensorboard\"))\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=hyperparams[\"total_timesteps\"],\n",
        "            progress_bar=False,\n",
        "            callback=callbackList)\n",
        "\n",
        "# Save the trained model\n",
        "model.save(os.path.join(model_folder_path, \"final_model\"))\n",
        "\n",
        "env.close()\n",
        "env_val.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Get model path from last training job (uncomment if training job interrupted)\n",
        "# # List all entries in the directory\n",
        "# entries = os.listdir(training_data_path)\n",
        "\n",
        "# # Filter out only directories\n",
        "# folders = [entry for entry in entries if os.path.isdir(os.path.join(training_data_path, entry))]\n",
        "\n",
        "# # Sort the folders alphabetically\n",
        "# folders.sort()\n",
        "\n",
        "# # Get the last folder\n",
        "# model_folder_path = os.path.join(training_data_path, folders[-1])\n",
        "# print(model_folder_path)"
      ],
      "metadata": {
        "id": "DFZCWZfHlSv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Evaluation Atari Tennis environment with appropriate wrappers\n",
        "env_val = make_atari_env(env_str,\n",
        "                         n_envs=1,\n",
        "                         seed=1)\n",
        "env_val = VecFrameStack(env_val, n_stack=4)\n",
        "env_val = VecTransposeImage(env_val)\n",
        "\n",
        "# Load the best model\n",
        "best_model_path = os.path.join(model_folder_path, \"best_model\")\n",
        "best_model = DQN.load(best_model_path, env=env_val)\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(\n",
        "    best_model,\n",
        "    env_val,\n",
        "    n_eval_episodes=5)\n",
        "print(f\"Best Model - Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "best_metrics_path = os.path.join(log_dir, \"best_model_metrics.csv\")\n",
        "\n",
        "# Create Best Model Metrics file if not there\n",
        "if(not os.path.isfile(best_metrics_path)):\n",
        "  with open(best_metrics_path, 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"run_date\",\n",
        "                     \"batch_size\",\n",
        "                     \"learning_rate\",\n",
        "                     \"num_timesteps\",\n",
        "                     \"mean_reward\",\n",
        "                     \"std_reward\",\n",
        "                     \"n_envs\",\n",
        "                     \"gamma\",\n",
        "                     \"seed\"])\n",
        "\n",
        "new_data = [os.path.basename(os.path.normpath(model_folder_path)),\n",
        "            best_model.batch_size,\n",
        "            best_model.learning_rate,\n",
        "            best_model.num_timesteps,\n",
        "            mean_reward,\n",
        "            std_reward,\n",
        "            best_model.n_envs,\n",
        "            best_model.gamma,\n",
        "            best_model.seed]\n",
        "\n",
        "with open(best_metrics_path, 'a', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(new_data)\n",
        "\n",
        "# Record video of the best model playing Atari's Tennis\n",
        "rec_val = VecVideoRecorder(env_val,\n",
        "                           os.path.join(model_folder_path, \"videos\"),\n",
        "                           video_length=5000,\n",
        "                           record_video_trigger=lambda x: x == 0,\n",
        "                           name_prefix=\"best_model_{}\".format(name_prefix))\n",
        "\n",
        "obs = rec_val.reset()\n",
        "for _ in range(5000):\n",
        "    action, _states = best_model.predict(obs)\n",
        "    obs, rewards, dones, info = rec_val.step(action)\n",
        "    rec_val.render()\n",
        "    if dones:\n",
        "      break\n",
        "\n",
        "env_val.close()\n",
        "rec_val.close()"
      ],
      "metadata": {
        "id": "OqwJT0bhcOu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Model\n",
        "print(best_model.policy)"
      ],
      "metadata": {
        "id": "5omjtaubDH1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the evaluations.npz file\n",
        "data = numpy.load(os.path.join(model_folder_path, \"evaluations.npz\"))\n",
        "\n",
        "# Extract the relevant data\n",
        "timesteps = data['timesteps']\n",
        "results = data['results']\n",
        "\n",
        "# Calculate the mean and standard deviation of the results\n",
        "mean_results = numpy.mean(results, axis=1)\n",
        "std_results = numpy.std(results, axis=1)\n",
        "\n",
        "# Plot the results\n",
        "matplotlib.pyplot.figure()\n",
        "matplotlib.pyplot.plot(timesteps, mean_results)\n",
        "matplotlib.pyplot.fill_between(timesteps,\n",
        "                               mean_results - std_results,\n",
        "                               mean_results + std_results,\n",
        "                               alpha=0.3)\n",
        "\n",
        "matplotlib.pyplot.xlabel('Timesteps')\n",
        "matplotlib.pyplot.ylabel('Mean Reward')\n",
        "matplotlib.pyplot.title(f\"{rl_type} Performance on {env_str}\")\n",
        "matplotlib.pyplot.show()"
      ],
      "metadata": {
        "id": "Xisgp9SfcONO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}