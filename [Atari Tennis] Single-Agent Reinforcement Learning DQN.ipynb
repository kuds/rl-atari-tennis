{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMnjzwCWrEUAKMqw5h7zljv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuds/rl-atari-tennis/blob/main/%5BAtari%20Tennis%5D%20Single-Agent%20Reinforcement%20Learning%20DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Atari Tennis] Single-Agent Reinforcement Learning DQN"
      ],
      "metadata": {
        "id": "ob-2hGT3jNdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig"
      ],
      "metadata": {
        "id": "Fy1WJ1R-iP4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium gymnasium[atari] autorom"
      ],
      "metadata": {
        "id": "7U_qTvGlj2oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!AutoROM --accept-license"
      ],
      "metadata": {
        "id": "Le7dztPUkVz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3"
      ],
      "metadata": {
        "id": "NmOqw4UAmAoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import platform\n",
        "import torch\n",
        "import numpy\n",
        "import stable_baselines3\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "from stable_baselines3.common.vec_env import VecFrameStack\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.callbacks import CallbackList\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder\n",
        "from stable_baselines3.common.vec_env import VecTransposeImage\n",
        "import matplotlib.pyplot\n",
        "import matplotlib\n",
        "import os\n",
        "import gymnasium\n",
        "from importlib.metadata import version\n",
        "from datetime import datetime\n",
        "import csv\n",
        "import time\n",
        "import google.colab.drive"
      ],
      "metadata": {
        "id": "o89JlEiexh1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Python Version: {platform.python_version()}\")\n",
        "print(f\"Torch Version: {version('torch')}\")\n",
        "print(f\"Is Cuda Available: {torch.cuda.is_available()}\")\n",
        "print(f\"Cuda Version: {torch.version.cuda}\")\n",
        "print(f\"Gymnasium Version: {version('gymnasium')}\")\n",
        "print(f\"Numpy Version: {version('numpy')}\")\n",
        "print(f\"Stable Baselines3 Version: {version('stable_baselines3')}\")"
      ],
      "metadata": {
        "id": "KRrJSPuDiDN4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "212a75ed-8b8b-417e-c824-e775fe8ffd4f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python Version: 3.10.12\n",
            "Torch Version: 2.4.1+cu121\n",
            "Is Cuda Available: True\n",
            "Cuda Version: 12.1\n",
            "Gymnasium Version: 0.29.1\n",
            "Numpy Version: 1.26.4\n",
            "Stable Baselines3 Version: 2.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gdrive_path = \"/content/gdrive\"\n",
        "google.colab.drive.mount(gdrive_path, force_remount=True)"
      ],
      "metadata": {
        "id": "LKxm5e7iGW_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rl_type = \"DQN\"\n",
        "env_str = \"ALE/Tennis-v5\"\n",
        "trained_model = \"final_atari_tennis_dqn\"\n",
        "log_dir = \"{}/MyDrive/Finding Theta/logs/{}/{}\".format(gdrive_path,\n",
        "                                                       env_str,\n",
        "                                                       rl_type)\n",
        "training_data_path = os.path.join(log_dir, \"training jobs\")\n",
        "time_folder = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "model_folder_path = os.path.join(log_dir, \"training jobs\", time_folder)"
      ],
      "metadata": {
        "id": "aR7w3BqcFiIO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Folders\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "os.makedirs(training_data_path, exist_ok=True)\n",
        "os.makedirs(model_folder_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "TKSfoDHMdbsf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = make_atari_env(env_str, n_envs=1, seed=0)\n",
        "print(\"Observation Space Size: \", env.observation_space.shape)\n",
        "print('Actions Space: ', env.action_space)\n",
        "env.close()"
      ],
      "metadata": {
        "id": "T78JvLBVdmQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gymnasium.make(env_str)\n",
        "print(\"Observation Space Size: \", env.observation_space.shape)\n",
        "print('Actions Space: ', env.action_space)\n",
        "env.close()"
      ],
      "metadata": {
        "id": "5vCHAlsgg3lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Environment Wrapper Arguments\n",
        "# Disable Frame Skipping\n",
        "# https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\n",
        "# https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.html\n",
        "env_val_wrap_args = {\"frame_skip\": 0, \"noop_max\": 30}"
      ],
      "metadata": {
        "id": "1bgRLf7cUY-8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoRecordCallback(BaseCallback):\n",
        "    def __init__(\n",
        "        self,\n",
        "        save_path: str,\n",
        "        video_length: int,\n",
        "        save_freq: int = 5_000,\n",
        "        name_prefix: str =\"rl_model\",\n",
        "        verbose: int = 0):\n",
        "\n",
        "        super().__init__(verbose)\n",
        "        self.save_freq = save_freq\n",
        "        self.video_length = video_length\n",
        "        self.save_path = save_path\n",
        "        self.name_prefix = name_prefix\n",
        "        # Those variables will be accessible in the callback\n",
        "        # (they are defined in the base class)\n",
        "        # The RL model\n",
        "        # self.model = None  # type: BaseAlgorithm\n",
        "        # An alias for self.model.get_env(), the environment used for training\n",
        "        # self.training_env # type: VecEnv\n",
        "        # Number of time the callback was called\n",
        "        # self.n_calls = 0  # type: int\n",
        "        # num_timesteps = n_envs * n times env.step() was called\n",
        "        # self.num_timesteps = 0  # type: int\n",
        "        # local and global variables\n",
        "        # self.locals = {}  # type: Dict[str, Any]\n",
        "        # self.globals = {}  # type: Dict[str, Any]\n",
        "        # The logger object, used to report things in the terminal\n",
        "        # self.logger # type: stable_baselines3.common.logger.Logger\n",
        "        # Sometimes, for event callback, it is useful\n",
        "        # to have access to the parent object\n",
        "        # self.parent = None  # type: Optional[BaseCallback]\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.save_freq == 0:\n",
        "\n",
        "          name_prefix = f\"{self.name_prefix}_{self.num_timesteps}\"\n",
        "\n",
        "          # Record video of the best model playing Atari's Tennis\n",
        "          rec_val = make_atari_env(\n",
        "              env_str,\n",
        "              n_envs=1,\n",
        "              seed=1,\n",
        "              wrapper_kwargs=env_val_wrap_args)\n",
        "          rec_val = VecTransposeImage(rec_val)\n",
        "          rec_val = VecFrameStack(rec_val, n_stack=4)\n",
        "          rec_val = VecVideoRecorder(rec_val,\n",
        "                                    self.save_path,\n",
        "                                    video_length=self.video_length,\n",
        "                                    record_video_trigger=lambda x: x == 0,\n",
        "                                    name_prefix=name_prefix)\n",
        "\n",
        "          obs = rec_val.reset()\n",
        "          for _ in range(self.video_length):\n",
        "              action, _states = self.model.predict(obs)\n",
        "              obs, rewards, dones, info = rec_val.step(action)\n",
        "              rec_val.render()\n",
        "              if dones:\n",
        "                break\n",
        "\n",
        "          rec_val.close()\n",
        "        return True"
      ],
      "metadata": {
        "id": "XxdvhhHvw-1O"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Environments\n",
        "# Create the Training Atari Tennis environment with appropriate wrappers\n",
        "env = make_atari_env(env_str, n_envs=4)\n",
        "env = VecTransposeImage(env)\n",
        "env = VecFrameStack(env, n_stack=4)\n",
        "\n",
        "# Create the Evaluation Atari Tennis environment with appropriate wrappers\n",
        "# Disable Frame Skip\n",
        "env_val = make_atari_env(\n",
        "    env_str,\n",
        "    n_envs=1,\n",
        "    seed=1,\n",
        "    wrapper_kwargs=env_val_wrap_args)\n",
        "env_val = VecTransposeImage(env_val)\n",
        "env_val = VecFrameStack(env_val, n_stack=4)"
      ],
      "metadata": {
        "id": "OxXLoo7erYVa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Callbacks\n",
        "# Create Evaluation Callback\n",
        "# eval_freq - can cause learning instability if set to low\n",
        "eval_callback = EvalCallback(\n",
        "    env_val,\n",
        "    best_model_save_path=model_folder_path,\n",
        "    log_path=model_folder_path,\n",
        "    eval_freq=5_000,\n",
        "    render=False,\n",
        "    deterministic=True,\n",
        "    n_eval_episodes=5)\n",
        "\n",
        "# Create Checkpoint Callback\n",
        "checkpoint_callback = CheckpointCallback(\n",
        "    save_freq=5_000,\n",
        "    save_path=os.path.join(model_folder_path, \"checkpoints\"),\n",
        "    name_prefix=\"atari_tennis_dqn\",\n",
        "    save_replay_buffer=False,\n",
        "    save_vecnormalize=False,\n",
        ")\n",
        "\n",
        "video_record_callback = VideoRecordCallback(\n",
        "    save_path=os.path.join(model_folder_path, \"videos\"),\n",
        "    video_length=2_500,\n",
        "    save_freq=5_000,\n",
        "    name_prefix=\"atari_tennis_dqn\")\n",
        "\n",
        "# Create the callback list\n",
        "callbackList = CallbackList([checkpoint_callback, video_record_callback, eval_callback])"
      ],
      "metadata": {
        "id": "_wLI5Z46rstT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvPXWTCHiO69"
      },
      "outputs": [],
      "source": [
        "# Initialize DQN\n",
        "model = DQN('CnnPolicy', env,\n",
        "            verbose=0,\n",
        "            batch_size=128,\n",
        "            buffer_size=750_000,\n",
        "            tensorboard_log=os.path.join(log_dir, \"tensorboard\"))\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=10_000_000,\n",
        "            progress_bar=False,\n",
        "            callback=callbackList)\n",
        "\n",
        "# Save the trained model\n",
        "model.save(os.path.join(model_folder_path, trained_model))\n",
        "\n",
        "env.close()\n",
        "env_val.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Get model path from last training job (uncomment if training job interrupted)\n",
        "# # List all entries in the directory\n",
        "# entries = os.listdir(training_data_path)\n",
        "\n",
        "# # Filter out only directories\n",
        "# folders = [entry for entry in entries if os.path.isdir(os.path.join(training_data_path, entry))]\n",
        "\n",
        "# # Sort the folders alphabetically\n",
        "# folders.sort()\n",
        "\n",
        "# # Get the last folder\n",
        "# model_folder_path = os.path.join(training_data_path, folders[-1])\n",
        "# print(model_folder_path)"
      ],
      "metadata": {
        "id": "DFZCWZfHlSv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Evaluation Atari Tennis environment with appropriate wrappers\n",
        "# Disable Frame Skip\n",
        "env_val = make_atari_env(env_str,\n",
        "                         n_envs=1,\n",
        "                         seed=1,\n",
        "                         wrapper_kwargs=env_val_wrap_args)\n",
        "env_val = VecTransposeImage(env_val)\n",
        "env_val = VecFrameStack(env_val, n_stack=4)\n",
        "\n",
        "# Load the best model\n",
        "best_model_path = os.path.join(model_folder_path, \"best_model\")\n",
        "best_model = DQN.load(best_model_path, env=env_val)\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(\n",
        "    best_model,\n",
        "    env_val,\n",
        "    n_eval_episodes=5)\n",
        "print(f\"Best Model - Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "best_metrics_path = os.path.join(log_dir, \"best_model_metrics.csv\")\n",
        "\n",
        "# Create Best Model Metrics file if not there\n",
        "if(not os.path.isfile(best_metrics_path)):\n",
        "  with open(best_metrics_path, 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"run_date\",\n",
        "                     \"batch_size\",\n",
        "                     \"learning_rate\",\n",
        "                     \"num_timesteps\",\n",
        "                     \"mean_reward\",\n",
        "                     \"std_reward\",\n",
        "                     \"n_envs\",\n",
        "                     \"gamma\",\n",
        "                     \"seed\"])\n",
        "\n",
        "new_data = [os.path.basename(os.path.normpath(model_folder_path)),\n",
        "            best_model.batch_size,\n",
        "            best_model.learning_rate,\n",
        "            best_model.num_timesteps,\n",
        "            mean_reward,\n",
        "            std_reward,\n",
        "            best_model.n_envs,\n",
        "            best_model.gamma,\n",
        "            best_model.seed]\n",
        "\n",
        "with open(best_metrics_path, 'a', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(new_data)\n",
        "\n",
        "# Record video of the best model playing Atari's Tennis\n",
        "rec_val = VecVideoRecorder(env_val,\n",
        "                           os.path.join(model_folder_path, \"videos\"),\n",
        "                           video_length=5000,\n",
        "                           record_video_trigger=lambda x: x == 0,\n",
        "                           name_prefix=\"best_model_atari_tennis_dqn\")\n",
        "\n",
        "obs = rec_val.reset()\n",
        "for _ in range(5000):\n",
        "    action, _states = best_model.predict(obs)\n",
        "    obs, rewards, dones, info = rec_val.step(action)\n",
        "    rec_val.render()\n",
        "    if dones:\n",
        "      break\n",
        "\n",
        "env_val.close()\n",
        "rec_val.close()"
      ],
      "metadata": {
        "id": "OqwJT0bhcOu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Model\n",
        "print(best_model.policy)"
      ],
      "metadata": {
        "id": "5omjtaubDH1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the evaluations.npz file\n",
        "data = numpy.load(os.path.join(model_folder_path, \"evaluations.npz\"))\n",
        "\n",
        "# Extract the relevant data\n",
        "timesteps = data['timesteps']\n",
        "results = data['results']\n",
        "\n",
        "# Calculate the mean and standard deviation of the results\n",
        "mean_results = numpy.mean(results, axis=1)\n",
        "std_results = numpy.std(results, axis=1)\n",
        "\n",
        "# Plot the results\n",
        "matplotlib.pyplot.figure()\n",
        "matplotlib.pyplot.plot(timesteps, mean_results)\n",
        "matplotlib.pyplot.fill_between(timesteps,\n",
        "                               mean_results - std_results,\n",
        "                               mean_results + std_results,\n",
        "                               alpha=0.3)\n",
        "\n",
        "matplotlib.pyplot.xlabel('Timesteps')\n",
        "matplotlib.pyplot.ylabel('Mean Reward')\n",
        "matplotlib.pyplot.title(f\"{rl_type} Performance on {env_str}\")\n",
        "matplotlib.pyplot.show()"
      ],
      "metadata": {
        "id": "Xisgp9SfcONO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "If3saIYUlM7h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}